<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Aradinka | Digital Garden on</title><link>https://aradinka.github.io/</link><description>Recent content in Aradinka | Digital Garden on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://aradinka.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://aradinka.github.io/tags/authors-ali-abdaal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/tags/authors-ali-abdaal/</guid><description/></item><item><title>_areas</title><link>https://aradinka.github.io/_areas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/_areas/</guid><description>Productivity</description></item><item><title>_authors</title><link>https://aradinka.github.io/_authors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/_authors/</guid><description>Ali Abdaal</description></item><item><title>batch-size</title><link>https://aradinka.github.io/batch-size/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/batch-size/</guid><description>Baca tentang [[batch-nomalization]]
Batch size salah satu yang bisa kita tuning. Tuning batch sizenya sampai kita mendapatkan performa yang maksimum</description></item><item><title>bias</title><link>https://aradinka.github.io/bias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/bias/</guid><description> Bias is a learner’s tendency to consistently learn the same wrong thing</description></item><item><title>bias-vs-variance</title><link>https://aradinka.github.io/bias-vs-variance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/bias-vs-variance/</guid><description>It provides another perspective to look at the phenomenon of [[underfitting]] and [[overfitting]]
[[bias]] is a learner’s tendency to consistently learn the same wrong thing</description></item><item><title>cheat-sheet</title><link>https://aradinka.github.io/cheat-sheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/cheat-sheet/</guid><description>ML Algorithm Datacamp https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet
Microsoft Azure https://learn.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet</description></item><item><title>classification</title><link>https://aradinka.github.io/classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/classification/</guid><description>Why we can’t do a classification problem using Regression? With linear regression you fit a polynomial through the data - say, like on the example below, we fit a straight line through {tumor size, tumor type} sample set:</description></item><item><title>convolutional-neural-network</title><link>https://aradinka.github.io/convolutional-neural-network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/convolutional-neural-network/</guid><description>Kenapa cnn bagus untuk dipakai di data image, text, audio dan video?
Mencerminkan spatial correlation between features Di data image: Pixel di titik i, j sangat berkaitan dengan i+1, j+1.</description></item><item><title>decision-tree</title><link>https://aradinka.github.io/decision-tree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/decision-tree/</guid><description>What is decision tree? A decision tree is a tree-shaped model guiding us in which order to check the features of an object, to output its discrete or continuous label.</description></item><item><title>difference-between-logistic-and-linear-regression</title><link>https://aradinka.github.io/difference-between-logistic-and-linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/difference-between-logistic-and-linear-regression/</guid><description>Linear and Logistic regression are the most basic form of regression which are commonly used.
Linear Logistic dependent variable is continuous dependent variable is binary requires to establish the linear relationship among dependent and independent variables not necessary for logistic regression independent variable can be correlated with each other the variable must not be correlated with each other</description></item><item><title>ds-ml-step</title><link>https://aradinka.github.io/ds-ml-step/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/ds-ml-step/</guid><description>ML steps:
determine which type of ML problems we would like to solve gather data [[feature-engineering]] Reference</description></item><item><title>ds-skill</title><link>https://aradinka.github.io/ds-skill/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/ds-skill/</guid><description> [[feature-engineering]] [[hyper-parameter-tuning]]</description></item><item><title>entropy</title><link>https://aradinka.github.io/entropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/entropy/</guid><description> varies from 0 to 1 0: all the data belong to a single class 1: the class distribution is equal a measure of the amount of uncertainity in the data set when $H(S)=0$, the set is perfectly classified (all element in $S$ are of the same class)</description></item><item><title>feature-engineering</title><link>https://aradinka.github.io/feature-engineering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/feature-engineering/</guid><description>a group of activities that trasnform data into desired format
example:
splitting data into training and testing handle missing values encode categorical attributes the outermost interface that we interact with the model</description></item><item><title>General Architechture of Machine Learning</title><link>https://aradinka.github.io/general-architechture-of-machine-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/general-architechture-of-machine-learning/</guid><description>Business Understanding: Understand the give use case and also it&amp;rsquo;s good to know more about the domain for which the use cases are built.</description></item><item><title>great-videos</title><link>https://aradinka.github.io/great-videos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/great-videos/</guid><description>inside-ali-abdaal-second-brain</description></item><item><title>hyper-parameter-tuning</title><link>https://aradinka.github.io/hyper-parameter-tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/hyper-parameter-tuning/</guid><description>The reason that it is highlighted as &amp;lsquo;hyper&amp;rsquo; is because the parameters that we tune are the outermost interface that we interact with the model, which would eventually have impacts on the underlying parameters of the model</description></item><item><title>impurity</title><link>https://aradinka.github.io/impurity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/impurity/</guid><description>impurity is a measure of homogeneity of the labels at the node at hand
Define impurity:
There are different ways to define impurity.</description></item><item><title>inside-ali-abdaal-second-brain</title><link>https://aradinka.github.io/inside-ali-abdaal-second-brain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/inside-ali-abdaal-second-brain/</guid><description>https://youtu.be/-Y_6U0FoqDk</description></item><item><title>logistic-regression</title><link>https://aradinka.github.io/logistic-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/logistic-regression/</guid><description>The logistic regression technique involves the dependent variable, which can be represented in the binary (0 or 1, true or false, yes or no) values, which means that the outcome could only be in either one form of two.</description></item><item><title>loss-function</title><link>https://aradinka.github.io/loss-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/loss-function/</guid><description>the cost that is incurred by the difference between the prediction and the true value. The larger the difference, the bigger the loss.</description></item><item><title>machine-learning-algorithm</title><link>https://aradinka.github.io/machine-learning-algorithm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/machine-learning-algorithm/</guid><description>A machine learning algorithm is the process that uncovers the underlying relationship whith the data
The outcome of a machine learning algorithm is called [[machine-learning-model]], which can be considered as a function $F$, which outputs certain results, when given the input.</description></item><item><title>machine-learning-model</title><link>https://aradinka.github.io/machine-learning-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/machine-learning-model/</guid><description>Rather than a predifined and fixed function, a machine learning model is dedrived from historical data. Therefore, when fed with with different data, the output of [[machine-learning-algorithm]] changes, i.</description></item><item><title>main-prediction</title><link>https://aradinka.github.io/main-prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/main-prediction/</guid><description>a learner is playing a dart-throwing game
how many points that the player would score?
https://leetcode.</description></item><item><title>overfitting</title><link>https://aradinka.github.io/overfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/overfitting/</guid><description>An overfitting model is the one that fits well with the training data, i.e. little or no error, however it does not generalized well to the unseen data</description></item><item><title>random-forest</title><link>https://aradinka.github.io/random-forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/random-forest/</guid><description>Advantages &amp;amp; Disadvantages Advantages Reduces [[overfitting]] Higher accuracy compared to other models Random Forests address both [[decision-tree]] issues ([[overfitting]] and instability).</description></item><item><title>regularization</title><link>https://aradinka.github.io/regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/regularization/</guid><description>penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data</description></item><item><title>semi-supervised-learning</title><link>https://aradinka.github.io/semi-supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/semi-supervised-learning/</guid><description>The data set is massive but the labeled sample are few
Training strategy:
we train a model with the labeled data, then we apply the model to predict the unlabeled data first cluster the images into groups (unsupervised learning), and then apply the supervised learning algorithm on each of the groups individually Reference</description></item><item><title>supervised-learning</title><link>https://aradinka.github.io/supervised-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/supervised-learning/</guid><description>An important measurement for supervised learning algorithms, is the generalization, which measures how well that a model derived from the training data can predict the desired attribute of the unseen data.</description></item><item><title>Tags System</title><link>https://aradinka.github.io/tags-system/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/tags-system/</guid><description>PARA Methods P.A.R.A. stands for Projects — Areas — Resources — Archives, the four top-level categories that encompass every type of information you might encounter in your work and life.</description></item><item><title>tags-list</title><link>https://aradinka.github.io/tags-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/tags-list/</guid><description> all-post algorithm what-is</description></item><item><title>underfitting</title><link>https://aradinka.github.io/underfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/underfitting/</guid><description>An underfitting model is the one that does not fit well with the training data, i.e. significantly deviated from the ground truth</description></item><item><title>underfitting-overfitting</title><link>https://aradinka.github.io/underfitting-overfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/underfitting-overfitting/</guid><description>When we say a model is underfitting or overfitting, it implies that the model does not generalized well to the unseen data.</description></item><item><title>variance</title><link>https://aradinka.github.io/variance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/variance/</guid><description> variance is the tendency to learn random things unrelated to the real signal</description></item><item><title>What is l1 regularization?</title><link>https://aradinka.github.io/l1-regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/l1-regularization/</guid><description>The main objective of creating a model (training data) is making sure it fits the data properly and reduce the loss.</description></item><item><title>What is l2 regularization?</title><link>https://aradinka.github.io/l2-regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/l2-regularization/</guid><description>Overfitting happens when the model learns signal as well as noise in the training data and wouldn&amp;rsquo;t perform well on new/unseen data on which model wasn&amp;rsquo;t trained on.</description></item><item><title>What is linear regression?</title><link>https://aradinka.github.io/linear-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/linear-regression/</guid><description>Linear Regressions Linear regressions tends to establish a relationship between a dependent variable Y and one or more independent variable X by finding the best fit of the straight line.</description></item><item><title>What is machine learning?</title><link>https://aradinka.github.io/machine-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/machine-learning/</guid><description>Machine learning is the scientific study of algorithms and statistical models that computer systems use to affectively perform a specific task without using explicit instruction, relyring on patterns and inference instead.</description></item><item><title>What is mean square error?</title><link>https://aradinka.github.io/mean-square-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/mean-square-error/</guid><description>The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distance from the points to the regression line (these distance are the &amp;ldquo;errors&amp;rdquo;) and squaring them.</description></item><item><title>What is ordinary least square model?</title><link>https://aradinka.github.io/ordinary-least-square-model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/ordinary-least-square-model/</guid><description>OLS (Ordinary Least Square) is a stats model, which will help us in identifying the more significant features that can has an influence on the output.</description></item><item><title>What is R squared?</title><link>https://aradinka.github.io/r-squared/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/r-squared/</guid><description>R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.</description></item><item><title>What is support vector regression?</title><link>https://aradinka.github.io/support-vector-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/support-vector-regression/</guid><description>Why SVR (support vector regression)? What is the difference between SVR and a simple regression model?
The answer is, simple linear regression try to minimize the error rate.</description></item><item><title>What is the difference between AI, DS, ML and DL?</title><link>https://aradinka.github.io/difference-between-ai-ds-ml-dl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/difference-between-ai-ds-ml-dl/</guid><description>Artificial Intelligence AI is purely math and scientifc exercise, but when it became computational, it started to solve human problems formalized into [[a subset of computer science]].</description></item><item><title>What is the difference between supervised, unsupervised, and reinforcement learning?</title><link>https://aradinka.github.io/difference-between-supervised-unsupervised-reinforcement-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aradinka.github.io/difference-between-supervised-unsupervised-reinforcement-learning/</guid><description>Supervised Learning In a supervised learning model, the algorithm learns on a labeled dataset, to generate reasonable predictions for the response to new data.</description></item></channel></rss>