{"/":{"title":"Aradinka | Digital Garden","content":"\nThis website meant to be my digital garden built using [Quartz](https://github.com/jackyzha0/quartz).\n\nCheck out my portfolio **[aradinka.com](https://aradinka.com)**\n\n[All Post](/tags/all-post)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/_areas":{"title":"_areas","content":"\n[Productivity](/tags/areas-productivity)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/_authors":{"title":"_authors","content":"\n[Ali Abdaal](/tags/authors-ali-abdaal)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ab-testing":{"title":"ab-testing","content":"\n## Overview\n\n\u003e A/B testing is a popular way to test your products. A/B testing is one of the most prominent and widely used statistical tools.\n\n## Example Scenario\n\n\u003e  You have made certain changes to your website recently. Unfortunately, **you have no way of knowing with full accuracy how the next 100,000 people who visit your website will behave**. That is the information we cannot know today, and if we were to wait until those 100,000 people visited our site, it would be too late to optimize their experience.\n\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/09/ab_test.png)\n\nHere A will remain unchanged while you make significant changes in B’s packaging. Now, on the basis of the response from customer groups who used A and B respectively, you try to decide which is performing better.\n\n## What is A/B testing?\n\nA/B testing is a basic **randomized control experiment**. It is a way to compare the two version of a variable to find out which performs better **in a controlled environment**.\n\nIt's a hypothetical testing methodology for makin decisions that estimate population parameters based on sample statistics.\n- Population: All customers buying the product\n- Sample: The number of customers that participated the test\n\n## Steps\n\n### 1. Make a Hypothesis\n\n\u003e A hypothesis is a tentative insight into the natural world\n\nA [[hypothesis-testing]] is a concept that is not yet verified, but if true would explain certain facts or phenomena.\n\nIn this example, the hypothesis can be \"By making changes in the design of the new page in the website, we can get more traffic on the website\"\n\n\nNull hypothesis $H_0:$\n- $H_0:$ There is no difference in the conversion rate in customer receiving pages A and B\n\nAlternative hypothesis $H_1:$\n- $H_1:$ The conversion rate of pages B is higher than those who receive newsletter A\n\n### 2. Create Control Group and Test Group\n\n\u003e Dedcide the group of customers that wil participate in the test.\n\nThe control group is the one that will receive newsletter A and the test group is the one will receive newsletter B.\n\nFor an experiment, we randomly select 1000 cutomers using [[random-sampling]]. 500 each for control group and testt group.\n\n**We must take care of the sample size**. It's required that we determine the minimum sample size for our A/B testing before confucting it so that we can eliminate [[under-coverage-bias]]. It's the bias from smapling too few observations.\n\n### 3. Conduct A/B Test and Collect the Data\n\nOne way to perform the test is to calculate daily [[conversion-rate]] for both the treatment and the control groups. Since the [[conversion-rate]] in a group on a certain day represents a single datat point, **the sample size is actually the number of days**. Thus, **we will be testing the difference between the mean of daily conversion rates in each group** across the testing period.\n\nWhen we run our experiment for one month, we noticed that the mean [[conversion-rate]] for the control group is **16%** whereas that for the test group is **19%**.\n\n\u003e Can we conclude from here that the Test group is working better than the control group? -\u003e Use statistical test\n\n## Statistical Significance of the Test\n\nFor rejecting out null hypothesis we have to prove the **statistical significance** of our test.\n\nAn experiment is considered to be statistically significant when we have enough evidence to prove that the result we see in the sample **also exist in population**.\n\nTo prove the statistical significance of the experiment, we can use two sample [[t-test]].\n\n## Code\n\n### Check whether 2 groups are similar or not\n\nSteps:\n- Split and define control \u0026 test group\n- Check normality: Shapiro test (pval \u003e 0.05 = normal)\n- If group A \u0026 B normal -\u003e check homogeneity of variance using levene test (pval \u003e 0.05 = homogen) -\u003e ttest (H0: M1 == M2)\n- If one of A / B not normal -\u003e non-parametric mann whitney u test\n\nCode sample\n```python\nfrom scipy.stats import shapiro\nimport scipy.stats as stats\n\ndf[\"version\"] = np.where(df.version == \"gate_30\", \"A\", \"B\")\n\ngroup = \"version\"\ntarget = \"sum_gamerounds\"\n\ngroupA = df[df[group] == \"A\"][target]\ngroupB = df[df[group] == \"B\"][target]\n\n# Normality Assumption, \n# H0: Fulfilled\n# H1: Not Fultilled\n\nntA = shapiro(groupA)[1] \u003c 0.05\nntB = shapiro(groupB)[1] \u003c 0.05\n\nif (ntA == False) \u0026 (ntB == False): # Normality Fulfilled, # Parametric Test\n    # Assumption: Homogeneity of Variance\n    # H0: homogeinety: False\n    # H1: Heterogenerous: True\n    leveneTest = stats.levene(groupA, groupB)[1] \u003c 0.05\n    \n    if leveneTest == False:\n        # Homogeneity\n        # H0: M1 == M2: False\n        # H1: M1 != M2: True\n        ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n    else:\n        # Heterogenous\n        # H0: M1 == M2: False\n        # H1: M1 != M2: True\n        ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n\nelse: # Normality Violated, # Non-Parametric Test\n    # H0: M1 == M2: False\n    # H1: M1 != M2: True\n    ttest = stats.mannwhitneyu(groupA, groupB)[1]\n\nres = pd.DataFrame({\n    \"AB Hypothesis\": [ttest \u003c 0.05],\n    \"pvalue\": [ttest]\n})\n\nres[\"Test Type\"] = np.where((ntA == False) \u0026 (ntB == False), \"Parametric\", \"Non-Parametric\")\nres[\"AB Hypothesis\"] = np.where(res[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\nres[\"Comment\"] = np.where(res[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar\", \"A/B groups are not similar\")\n\nif (ntA == False) \u0026 (ntB == False):\n    res[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n    res = res[[\"Test Type\", \"Homogeneity\", \"AB Hypothesis\", \"pvalue\", \"Comment\"]]\nelse:\n    res = res[[\"Test Type\", \"AB Hypothesis\", \"pvalue\", \"Comment\"]]\nres\n```\n\n\n\n\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/activation-function":{"title":"activation function","content":"\nActivation function are important for an ANN to learn and make sense of something complicated like non-linear complex functional mappings between the inputs and response variable.\n\nMost popular activation function:\n- [[sigmoid]]\n- [[tanh]]\n- [[relu]]\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/agglomerative-hierarchical-clustering":{"title":"agglomerative-hierarchical-clustering","content":"\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/analysis-step":{"title":"analysis-step","content":"\n- check `.columns`\n- `.describe()` data\n\t- check missing values\n\t- check mean \u0026 [[standard-deviation]] values\n\t- check min, percentil, max valeus\n- ","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/back-propagation":{"title":"back propagation","content":"\nBackpropagation:\n- Is a short form of \"backward propagation of errors\"\n- It's the standard method of training [[artificial-neural-network]]\n- It helps to calculate the [[gradient]] of a [[loss-function]] with respects to all the weights in the network\n- This method of fine-tuning the weights of a neural net based on the errors rate obtained in the previous epoch.\n- Proper tuning of the weights -\u003e Allow us to reduct error rates -\u003e make the model reliable by increasing it's generalisation\n\nAdvantages:\n- Fast, simple, and easy to program\n- It has no parameters to tune apart from the num of input\n- It doesn't need any special mentions of the features of the function to be learned\n\nAlgorithm:\nFor each sample in the training set...\n- compute the output signal\n- compute the error corresponding to the output level\n- propagate the error back into the network and store the corresponding delta values for each layer\n- adjust each weight by using the error signal and input signal for each layer\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/batch-size":{"title":"batch-size","content":"\nBaca tentang [[batch-nomalization]]\n\nBatch size salah satu yang bisa kita tuning. Tuning batch sizenya sampai kita mendapatkan performa yang maksimum \n\nPengaruh batch size pada performa model:\n- Terlalu kecil: Distribusi hasil batch normalization kurang bagus. Kadang skew ke kiri kadang ke kanan\n- Terlalu besar: Hessian matrix tidak mencerminkan local minima\n\n- Apabila menggunakan batch size besar, gunakan [[learning-rate]] yang kecil\n\n\n[Innovation Day: MultiGPU Infrastructure \u0026 Implementation](https://youtu.be/IayDLHyHqlE?t=3574)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/bias":{"title":"bias","content":"\nBias is a learner’s tendency to consistently learn the same wrong thing\n\nBias is the difference between the expected average prediction of the model, and the correct pvalue which we are trying to predict.\n\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/bias-vs-variance":{"title":"bias-vs-variance","content":"\nIt provides another perspective to look at the phenomenon of [[underfitting]] and [[overfitting]]\n\n\u003e [[bias]] is a learner’s tendency to consistently learn the same wrong thing\n\n\u003e [[variance]] is the tendency to learn random things unrelated to the real signal\n\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/cheat-sheet":{"title":"cheat-sheet","content":"\n\n# ML Algorithm\n\n## Datacamp\n\n![](cheat-sheet-ml-datacamp.jpg)\n\nhttps://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet\n\n## Microsoft Azure\n\n![](cheat-sheet-ml-microsoft-azure.png)\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/classification":{"title":"classification","content":"\n# Why we can’t do a classification problem using Regression?\n\nWith linear regression you fit a polynomial through the data - say, like on the example below, we fit a straight line through {tumor size, tumor type} sample set:\n\n![](images/classification-1.png)\n\nAbove, malignant tumors get 1, and non-malignant ones get 0, and the green line is our hypothesis $h(x)$. To make predictions, we may say that for any given tumor size x, if $h(x)$ gets bigger than 0.5, we predict malignant tumors. Otherwise, we predict benignly.\n\nIt looks like this way, we could correctly predict every single training set sample, but now let's change the task a bit.\n\n![](images/classification-2.png)\n\nNow our $h(x)\u003e0.5$→malignant doesn't work anymore. To keep making correct predictions, we need to change it to $h(x)\u003e0.2$ or something - but that not how the algorithm should work.\n\n**We cannot change the hypothesis each time a new sample arrives**. Instead, we should learn it off the training set data, and then (using the hypothesis we've learned) make correct predictions for the data we haven't seen before.\n\n\u003e **Linear regression is unbounded**\n\n\u003e Futher reading: [[linear-regression]], [[logistic-regression]]","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/code-screen":{"title":"code-screen","content":"\n# outside session\n\n```\n# create screen session\nscreen -S session_name\n\n# kill complete session\nscreen -S [session_name] -X quit\n\n# detatch running session\nscreen -d [session_name}]\n```\n\n# inside session\n\n```\n# create new window\nctrl a, c\n\n# list all window\nctrl a, \"\n\n# split horizontally\nctrl a, s\n\n# change between tab\nctrl a, tab\n\n# exit window\nctrl a, d\n```\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/code-sklearn":{"title":"code-sklearn","content":"\n```\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_absolute_error\n```","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/code-sql":{"title":"code-sql","content":"\n## Manipulation\n\n### Create a table with column constraints\n\n```sql\nCREATE TABLE table_name (\n\t-- Uniquely identify the row\n\tid INTEGER PRIMARY KEY,\n\t\n\t-- Columns have a different value for every row \n\tname TEXT UNIQUE\n\t\n\t-- Columns must have a value\n\tgrade INTEGER NOT NULL\n\t\n\t-- Assigns a default value when no value is specified\n\tage INTEGER DEFAULT 10\n);\n```\n\n### `INSERT` statement\n\n```sql\n-- Insert into columns in order\nINSERT INTO table_name\nVALUES (value1, value2);\n\n-- Insert into columns by name\nINSERT INTO table_name (column1, column2)\nVALUES (value1, value2);\n```\n\n### `ALTER TABLE` statement\n\n```sql\n-- Modify the columns of an existing table\n-- When combined with ADD clause, it used to add a new column\nALTER TABLE table_name\nADD column_name datatype;\n```\n\n### `DELETE` statement\n\n```sql\n-- Delete records (rows) in table\n-- WHERE clause specifies which records that should be deleted\n-- If WHERE clause omitted, all records will be deleted\nDELETE FROM table_name\nWHERE some_column = some_value;\n```\n\n### `UPDATE` statement\n\n```sql\n-- Edit records (rows) in table\n-- It includes a SET clause that indicate the column to edit\n-- and a WHERE clause for specifying the records\nUPDATE table_name\nSET column1 = value1, column2 = value2\nWHERE some_column = some_value;\n```\n\n\n## Reference\n\n- https://www.codecademy.com/learn","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/confusion-matrix":{"title":"confusion-matrix","content":"\n## Summary\n\nA good model has high TP and TN, while low FP(error 1 $\\alpha$) and FN(error 2 $\\theta$). Or F1 score has high value\n\nAccuracy:\n- How often the classifier makes correct predictions\n- It's the ratio (num of correct pred / total num of pred)\n- It's a useful metric when the true positives and true negatives are more important\n- Not suited for imbalanced data (when model predicts each point belongs to majority class label, the accuracy will be high but the model is not accurate)\n- A valid choice of evaluation for classification problem if the data are well balanced and not skewed.\n\nPrecision:\n- How many predictions are actually positive out of all the total positive predicted\n- It's the ratio (num of correctly classified positive / total num of predicted positive) TP/(TP+FP)\n- Ideally precision should be high\n- It's a useful metric in cases where **false positive is a higher concern** than false negative\n- Case 1: In spam detection we try to reduce FP (positive mail is not spam but predicted as spam)\n- Case 2: In recommendation systems, wrong result could lead to customer churn (good matched content predicted as bad content)\n\nRecall / sensitivity:\n- How many observations of positive class are actually predicted positive\n- It's the ratio (num of correctly classified positive / total num of actual positive) TP/(TP+FN) or (1 - error type 2)\n- A valid choice when we want to capture as many positives as possible\n- It's a useful metric in cases where **false negative trumps false positive**\n- It would be a better metric because we don't want to accidentally discharge an infected person.\n- Case 1: Medical cases where it doesn't matter whether we raise false alarm(FP), but the actual positive cases should not go undetected!(FN)\n- Case 2: A person suffering from cancer but the model predicted not suffering (FP)\n\nSpecificity:\n- It's the ratio (true negative / all negative outcomes) TN/(TN+FP)\n- It's useful if you concerned about the accuracy of your negative classes, and there is a high cost to a positive outcome\n- Case: Auditor looking over financial transactions, and a positive outcome would mean a one year investigation, but not finding one would only cost the company $50\n\nF1 Score:\n- It's a number between 0 and 1\n- It's the harmonic mean of precision and recall. It's maintains a balance\n- Harmonic mean is not sensitive to extremely large values (unlike simple average does). Harmonic mean punishes extreme values more\n- If your precision is low, the F1 is low. If the recall is low again, your F1 score is low\n- It's a better metric for imbalance data\n- It's useful when there is no clear distinction between whether precision is more important or recall is more important. We combine them!\n- When we try to increase precision, the recall goes down and vice-versa. F1 score capture both trends in a single value\n\n\n## What is confusion matrix?\n\nConfusion matrix is a table layout that allows visualization of the performance of an algorithm. It is an $N \\times N$ matrix used for evaluating the performance of a classification model, where $N$ is the number of target classes. The matrix compare the actual target values with those predicted by the model. It's a tabular summary of the number of correct and incorrect predictions made by a classifier.\n\nEach row represents the instances in an actual class, while each column represents the instances in a predicted class. It is a special kind of [[contingency-table]].\n\n![](https://miro.medium.com/max/720/1*jMs1RmSwnYgR9CsBw-z1dw.webp)\n\nIn unsupervised learning, it usually called a [[matching-matrix]] \n\nKey takeaway:\n- A good model is one which has high TP and TN rates, while low FP and FN rates\n- If you have an imbalance dataset, it's better to use confusion matrix as your evaluation criteria\n\n## Terminology\n\nP (Condiiton Positive)\n- The number of real positive cases in the data\n\nN (Condition Negative)\n- The number of real negative cases in the data\n\nTP (True Positive)\n- You predicted positive and it's true\n- When the actual value is positive and predicted is also positive\n- A test result that correctly indicates the presence of a condition or characteristic\n\nTN (True Negative)\n- You predicted negative and it's true\n- When the actual value is negative and predicted is also negative\n- A test result that correctly indicates the absence of a condition or characteristic\n\nFP (False Positive) / Type 1 error\n- You predicted positive and it's false\n- When the actual is negative but prediction is positive\n- A test result which wrongly indicates that a particular condition or atribute is present\n\nFN (False Negative) / Type 2 error\n- You predicted negative and it's false\n- When the actual is positive, but the prediction is negative\n- A test result which wrongly indicates that a particular condition or attribute is absent\n\n## Why we're not using only accuracy?\n\nIn classification accuracy, there is no information about the number of misclassified instances. It's not suited for imbalanced classes\n\n\n\n## Classification Measure\n\nAccuracy\n- How often the classifier makes the correct prediction\n- It's the ratio between the number of correct predictions and the total number of predictions- Not suited for imbalanced classes\n\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/conversion-rate":{"title":"conversion-rate","content":"\n## What is conversion rate?\n\n\u003e Conversion rate is often used by e-commerce sites to measure the **percentage of visitors that end up purchasing products**.\n\n\n### Formula\n\n**Conversion rate = (Conversions or goals achieved / Total visitors) * 100**\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/convolutional-neural-network":{"title":"convolutional-neural-network","content":"\nKenapa cnn bagus untuk dipakai di data image, text, audio dan video?\n- Mencerminkan spatial correlation between features\n- Di data image: Pixel di titik `i, j` sangat berkaitan dengan `i+1, j+1`. Dan sekitarnya1\n- Di data text: Text sebelum dan sesudah saling berhubungan\n\n[](https://youtu.be/IayDLHyHqlE?t=3725)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/decision-tree":{"title":"decision-tree","content":"\n## Summary\n\nIt's a tree-shaped supervised learning algorithm, works on if-then statement, that can be used in classification and regression problems. The input can be both continuous and categorical. Feature values are preferred to be categorical, if continuous they are discretized.\n\nAdvantages:\n- can handle missing values\n- explainable and interpretable\n\nDisadvantages:\n- prone to [[overfitting]]\n- sensitive to [[outlier]]\n\nUse Cases:\n- [[customer-churn-prediction]]\n- [[credit-score-modelling]]\n- [[disease-prediction]]\n\n\n### Overfitting and Instability of Decision Trees\n\n\nOverfitting:\n- Decision trees prone to [[overfitting]] the data. Since **accuracy improves with each internal node**, **training will tend to grow a tree to its maximum** to improve the performance metrics.\n- That deteriorates the tree’s generalization capability and usefulness on unseen data since it will start modeling the noise\n\nInstability:\n- We can limit the tree’s depth beforehand, but there’s still the problem of instability\n- even **small changes to the training data**, such as excluding a few instances, **can result in a completely different tree**\n\n## Depth of the tree\n\nWhen we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n\nOn the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups. \n\nAt an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason)\n\n A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n\n\n## Splitting the Data\n\n\u003e When splitting, we choose to partition the data by the attribute that results in the **smallest impurity** of the new nodes\n\n\n## Control overfitting in decision tree\n\n_max_leaf_nodes_ argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area to the overfitting area.\n\n## Algorithm\n\n\u003e A decision tree uses different algorithms to decide whether to split a node into two or more sub-nodes. **The algorithm chooses the partition maximizing the purity of the split (i.e., minimizing the [[impurity]])**\n\nimpurity is a measure of **homogeneity** of the labels at the node at hand\n\n### ID3 (Iterative Dichotomiser)\n\n- Uses [[entropy]] and [[information-gain]] as metrics to form a better decision tree\n- The attribute with the highest information gain is used as a root node, and a similar approach is followed after that \n- Entropy is the measure that characterizes the [[impurity]] of an arbitrary collection of examples\n- when entropy $H(S)=0$, the set is perfectly classified (all element in $S$ are of the same class)\n- when entropy $H(S)=1$, the class distribution is equal\n- entropy is calculated for each remaining attribute, the attribute with the smallest entropy is used to split the set $S$ on this iteration\n- the higher the entropy, the higher potential to improve the classification here\n\n![](entropy-1.png)\n\n\n## Reference\n\n- https://www.baeldung.com/cs/decision-trees-vs-random-forests\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/difference-between-ai-ds-ml-dl":{"title":"What is the difference between AI, DS, ML and DL?","content":"\n## Artificial Intelligence\n\n**AI is purely math and scientifc exercise**, but when it became computational, it started to solve human problems formalized into [[a subset of computer science]].\n\nAI has changed the original computational statistics paradigm to the modern idea that machines could mimic actual human capabilities, such as decision making and performing more \"human\" tasks.\n\nModern AI into two categories:\n1. **General AI**: Planning, decision making, identifying objects, recognizing sounds, social \u0026 business interactions\n2. **Applied AI**: Driverless / Autonomous car or machine smartly trade stocks\n\n## Machine Learning\n\nInstead of engineers \"teaching\" or programming computers to have what they need to carry out tasks, that perhaps computers could teach themselves - **learn something without being explicitly programmed to do so**.\n\n**ML (Machine Learning) Is a form of AI where based on more data**, and they can change actions and response, which will make more efficient, adaptable and scalable. E.g., navigation apps and recomendation engines.\n\nML Classifiec into:\n1. Supervised\n2. Unsupervised\n3. Reinforcement Learning\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n## Data Science\n\nData science has many tools, techniques and algorithms called from these fields to handle big data. The goal of data science, somewhat similar to machine learning, is to make accurate predictions and to automate and perform transactions in real-time, such as purchasing internet traffic or automatically generating content.\n\nData science relies less on math and coding and more on data and building new systems to process the data. Relying on the fields of data integration, distributed architechture, automated machine learning, data visualization, data engineering and automated data-driven decisions. Data science can cover an entire spectrum of data pprocessing, not only the algorithms or statistics related to data.\n\n## Deep Learning\n\nDeep learning is a technique for implementing ML. ML provides the desired output from a given input, but DL (Deep Learning) reads the input and applies it to another data. \n\nIn ML, we can easily classify the flower based upon the features. Suppose you want a machine to look at an image and determine what it represents to human eye, wherer a face, flower, landscape, truck, building, etc. ML is not sufficient for this task because machine learning can only produce an output from a data set - whether according to a known algorithm or based on the inherent structure of the data. You might be able to use machine learning to determine to use ML to determine whether an image was of an \"X\" - a flower, say - and it would learn and get more accurate. But that output is binary (yes/no) and is dependent on the algorithm, not the data.\n\nIn the image recognition case, the outcome is not binary and not dependent on the algorithm. The neural network perform micro calculations with computational on many layers. Neural networks also support weighting data for confidence. These results in  a probabilistic system, vs. deterministics, and can handle tasks that we think of as requiring more \"human-like\" judgement.Z\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/difference-between-logistic-and-linear-regression":{"title":"difference-between-logistic-and-linear-regression","content":"\nLinear and Logistic regression are the most basic form of regression which are commonly used. \n\n| Linear                                                                                  | Logistic                                            |\n|-----------------------------------------------------------------------------------------|-----------------------------------------------------|\n| dependent variable is continuous                                                        | dependent variable is binary                        |\n| requires to establish the linear relationship among dependent and independent variables | not necessary for logistic regression               |\n| independent variable can be correlated with each other                                  | the variable must not be correlated with each other |\n\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/difference-between-supervised-unsupervised-reinforcement-learning":{"title":"What is the difference between supervised, unsupervised, and reinforcement learning?","content":"\n## Supervised Learning\n\nIn a supervised learning model, the algorithm learns on **a labeled dataset**, to generate reasonable predictions for the response to new data. Ex:\n- Regression\n- Classification\n\n## Unsupervised Learning\n\nAn unsupervised learning model in contrast provides unlabelled data that the algorithm tries to make sense of by extracting features, co-occurrence and underlying patterns on its own. Ex:\n- Clustering\n- Anomaly detection\n- Associations\n- Autoencoders\n\n## Reinforcement Learning\n\nReinforcement learning is less supervised and depends on the learning agent in determining the output solutions by arriving at different possible ways to achieve the best possible solution.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds":{"title":"ds","content":"\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds-cover-letter":{"title":"ds cover letter","content":"\nDear hiring manager,\n\nI would like to introduce myself as an applicant for Data Scientist position at tiket.com. I am confident in my ability to perform data science related task at tiket.com due to my internship experience and my education background.\n\nDuring my internship at technical AI team at Telkom Indonesia, I had an extensive range of tasks including paper research, implement recent paper by reproducing the GitHub repo in our server, train language model with multiple GPU, built speaker diarization capability, produced custom 20GB+ audio conversation dataset and contributed to write paper. As a Data Scientist, I was required to have a good communication skill, good python skill, and undestanding the algorithms. During my one-year internship at Telkom Indonesia, I applied these skills daily.\n\nI would like to thank you for taking the time to review my application.\n\nSincerely,\n\nAzka Radinka\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds-hack":{"title":"ds-hack","content":"\n- save tabular files to parquet format [youtube](https://www.youtube.com/watch?v=u4rsA5ZiTls)\n- change string column to category\n- downcasting integers\n\t- int8 range : -128 to 127\n\t- int16 range: -32678 to 32676\n- downcasting float to float32 (check some values behind decimals)\n- change yes no column to boolean\n- speed up pandas df looping -\u003e apply() -\u003e vectorized","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds-interview":{"title":"ds-interview","content":"\n## Cover Letter\n\n\n## General Question\n\n- tell me about yourself\n- me in the next 5 years\n- kelebihan\n- kekurangan\n- source\n\t- https://www.linkedin.com/posts/ayutyas-pramodha-4637a416b_bei-ugcPost-7002190411498762240-MAG0?utm_source=share\u0026utm_medium=member_desktop\n\n\n##  Technical coding test\n- SQL -\u003e [[code-sql]]\n- Python\n\n## Past project detail\n- [[project-audio-conversation-transcription]]\n- [[project-madani]]\n- knowledge\n\t- clustering\n\t- cnn\n\t- time series\n\t- survival analysis\n\t- data viz\n\n## General ML question\n- confussion matrix\n\n## General stats question\n- statistical significance\n- [[hypothesis-testing]] testing\n\t- 2 types of error, the consequence, which one is better for what case\n- pvalue\n- distribution\n- t-test\n\n\n## General ML Algorithm\n- regression\n\t- linear\n\t- lasso\n\t- ridge\n- tree based\n\t- decision tree\n\t- random forest\n- unsupervised learning\n\t- clustering\n\t- association\n\n\n## Deep learning question \n- CNN\n- loss function\n- optimizer\n- forward backward prop","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds-ml-step":{"title":"ds-ml-step","content":"\nML steps:\n- determine which type of ML problems we would like to solve\n- gather data\n- [[feature-engineering]]\n- \n\n# Reference\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds-plotting":{"title":"ds-plotting","content":"\n## Read csv\n\n- parse_dates\n- index_col\n\n## Plotting in notebook setup\n\n```python\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n```\n\n## Define Your Viz Goals\n\n- Trends: lineplot\n- Relationship: scatterplot, lmplot,  regplot, heatmap, swarmplot \n- Distribution: histogram, kde, 2d kde\n- Compare values: barplot\n\n## Plot Option Based on Data Types\n\n- date time + multiple variable: line plot\n- 1 numeric: histogram, kde\n- 1 numeric multiple categorical: histogram\n- 2 numeric: scatterplot, 2d kde\n- 2 numeric + 1 categorical: scatterplot, lm regression plot\n- 1 categorical + 1 numeric: barplot, swarmplot\n- 2 categorical + 1 numeric: histogram, kde, barplot, heatmaps\n\n\n## Line Plot\n\n```python\nplt.figure(figsize=(14,6))\n\nplt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\n\nsns.lineplot(data=spotify_data['Shape of You'], label=\"Shape of You\")\nsns.lineplot(data=spotify_data['Despacito'], label=\"Despacito\")\n\nplt.xlabel(\"Date\")\n```\n\n## Bar Plot\n\n```python\nplt.figure(figsize=(14,7))\n\nsns.barplot(x=flight_data.index, y=flight_data['NK'])\nplt.ylabel(\"Arrival delay (in minutes)\")\n```\n\n## Heatmaps\n\n```python\nsns.heatmap(data=flight_data, annot=True)\n```\n\n## Scatter Plot\n\n```python\n# color by smoker categorical data\nsns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'], hue=insurance_data['smoker'])\n\n# add regression line\nsns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])\n# colored by smoker categorical data, create 2 regression line\nsns.lmplot(x=\"bmi\", y=\"charges\", hue=\"smoker\", data=insurance_data)\n```\n\n### Categorical Scatter Plot\n\n```python\n\n# smoker=category, charges=numeric\nsns.swarmplot(x=insurance_data['smoker'],\n              y=insurance_data['charges'])\n```\n\n## Histogram\n\n```python\nsns.histplot(data=iris_data, x='Petal Length (cm)', hue='Species')\n```\n\n### KDE (Kernel Density Estimation) Plot\n\n\u003e smoothed histogram\n\n```python\nsns.kdeplot(data=iris_data, x='Petal Length (cm)', hue='Species', shade=True)\n```\n\n### 2D KDE (Kernel Density Estimation) Plot\n\n```python\nsns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind=\"kde\")\n```","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/ds-skill":{"title":"ds-skill","content":"\n- read datas\n\t- check dtypes (handling datetime data, index_col)\n\t- check memory usage\n- summary data\n\t- summary distribution\n\t- summary between some categorical variable\n- [[feature-engineering]]\n\t- [[detect-extreme-values]]\n\t- [[handling-missing-values]]\n- [[hyper-parameter-tuning]]","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/entropy":{"title":"entropy","content":"\n\u003e In statistics, entropy is a measure of information\n\n![](entropy-1.png)\n\n- varies from 0 to 1\n\t- 0: all the data belong to a single class\n\t- 1: the class distribution is equal\n- a measure of the amount of uncertainity in the data set\n- when $H(S)=0$, the set is perfectly classified (all element in $S$ are of the same class)\n\n---\n\nLet’s assume that a dataset T associated with a node contains examples from n classes. Then, its entropy is:\n\n![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-004399bf736b5463087bdd15d256e30e_l3.svg)\n\nwhere $p_j$ is the relative frequency of class $j$ in $T$. \n\nAs is the case with the [[gini-impurity-index]], a node is pure when $entropy(T)$ takes its minimum value, zero, and impure when it takes its highest value, 1.\n\n## Example\n\n- 4 red, 0 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ba59773932c9932bd2af5ee6be657e67_l3.svg)\n- 2 red, 2 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-f8f46c401aa3e27c2e16351823618aa8_l3.svg)\n- 3 red, 1 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-6a910318c70392856b99697b48b0ecec_l3.svg)\n\n## Information Gain\n\n\u003e The information gain is the difference between a **parent node’s entropy** and **the weighted sum of its child node entropies**.\n\nLet’s assume a dataset $T$ with $N$ objects is partitioned into two datasets: $T_1$ and $T_2$ of sizes $N_1$ and $N_2$. **Then, the split’s Information Gain ($Gain_{split}$) is**:\n\n![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-652f1dd07771006ca61719507ae7784a_l3.svg)\n\nIn general, if splitting $T$ into m subsets $T_1, T_2, \\ldots, T_m$ with $N_1, N_2, \\ldots, N_m$ objects, respectively, the split’s Information Gain ($Gain_{split}$) is:\n\n![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-1c53327d676f878366ba10c245666a00_l3.svg)\n\n### Example Splitting by Information Gain\n\nhttps://www.baeldung.com/cs/impurity-entropy-gini-index#2-example-splitting-by-information-gain\n\nsteps:\n- determine the attribute that offers the highest Information Gain\n\n\n## References\n- https://www.baeldung.com/cs/impurity-entropy-gini-index\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/feature-engineering":{"title":"feature-engineering","content":"\n\u003e a group of activities that trasnform data into desired format\n\nexample:\n- splitting data into training and testing\n- handle missing values\n- encode categorical attributes\n\nthe outermost interface that we interact with the model\n# Reference\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/fitting":{"title":"fitting","content":"\n\u003e The step of capturing patterns from data is called **fitting**","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/gaussian-distribution":{"title":"gaussian distribution","content":"","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/general-architechture-of-machine-learning":{"title":"General Architechture of Machine Learning","content":"\n![](/images/general-architechture-of-machine-learning-1.png)\n\n**Business Understanding**: Understand the give use case and also it's good to know more about the domain for which the use cases are built.\n\n**Data Acquisition and Understanding**: Data gathering from different sources and understanding the data. Cleaning the data, handling the missing data if any, data wrangling, and EDA (Exploratory Data Analysis)\n\n\u003e Read More: [My Sample Material EDA Course on Sepuluh Nopember Institut of Technology](https://aradinka.vercel.app/blog/post-asdos)\n\n**Modeling**: Feature engineering, scaling the data, feature selection. Backward elimination method, correlation factors, PCA, and domain knowledge to select the features. Model training (Based on trial and error method of by experience, we select the algorithm and train with selected features). Model evaluation (Accuracy of the model, confusion matrix and cross-validation).\n\n**Deployment**: Once the model has good performance, we deploy the model in the cloud. Once we deploy, we monitor the performance of the model. If it's good, we go live with the model or reiterate all the process until our model performance is good.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/gini-impurity-index":{"title":"gini-impurity-index","content":"\n\u003e Gini Index is related to the **misclassification probability** of a random sample\n\nGini Index may result in values inside the interval [0, 0.5]\n- The minimum value of zero corresponds to **a node containing the elements of the same class**. In case this occurs, the node is called **pure**\n- The maximum value of 0.5 corresponds to the highest impurity of a node. Called **impure**\n\n# Example\n\nIn this example, we’ll compute the Gini Indices for 3 different cases of a set with 4 balls of two different colors, red and blue\n\n- 4 red, 0 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-03178ed050fa7f5ac3d42af8f8535842_l3.svg)\n- 2 red, 2 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-feb5ec6ce739f898c6c6ff05c2a9b904_l3.svg)\n- 3 red, 1 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-26774e1805b1b0048d64045c59ca7593_l3.svg)\n\n# Reference\n- https://www.baeldung.com/cs/impurity-entropy-gini-index","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/gradient-decent":{"title":"gradient descent","content":"\n- It's an iterative ML optimization algorithm\n- It reduce the cost function\n- It help models to make accurate predictions\n\nGradient indicates the **direction** of increase\n- As we want to find the minimum points in the valley, we need to go in the opposite direction of the gradient.\n\nTypes of gradient descents:\n- Batch / vanilla gradient descent\n- [[stochastic-gradient-descent]]\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/great-videos":{"title":"great-videos","content":"\n[inside-ali-abdaal-second-brain](inside-ali-abdaal-second-brain.md)\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/home":{"title":"home","content":"\n## Tasks\n\n- what is [[exploding-gradient]]\n- what is [[gaussian-distribution]]\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/hyper-parameter-tuning":{"title":"hyper-parameter-tuning","content":"\n\u003e The reason that it is highlighted as 'hyper' is because the parameters that we tune are **the outermost interface that we interact with the model**, which would eventually have impacts on the underlying parameters of the model\n\n\n","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/hypothesis-testing":{"title":"hypothesis","content":"\n## What is hypothesis?\n\n\u003e A hypothesis is a concept that is not yet verified, but if true would explain certain facts or phenomena\n\n## What is hypothesis testing?\n\nIt is an **educated guess** about something in the world around you. It should be **testable**, either by experiment or observation.\n\nIn hypothesis testing, we have to make two hypotheses i.e Null hypothesis and the alternative hypothesis\n\n### Null Hypothesis\n\nThe null hypothesis is the one that states that sample observations result purely from change.\n\n### Alternative Hypothesis\n\nThe alternative hypothesis challenges the null hypothesis and is basically **a hypothesis that the researcher believes to be true**.\n\n## Two types of error in hypothesis testing\n\n### Type I error\n\n\u003e We reject the null hypothesis when it is true\n\nThat is we accept the variant B when it is not performing better than A\n\n### Type II error\n\n\u003e We failed to reject the null hypothesis when it is false","lastmodified":"2022-12-02T14:20:10.404291813Z","tags":null},"/impurity":{"title":"impurity","content":"\n\u003e impurity is a measure of **homogeneity** of the labels at the node at hand\n\n\nDefine impurity:\n- There are different ways to define impurity. In classification tasks, we frequently use the [[gini-impurity-index]] and [[entropy]]","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/inside-ali-abdaal-second-brain":{"title":"inside-ali-abdaal-second-brain","content":"\nhttps://youtu.be/-Y_6U0FoqDk\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Y_6U0FoqDk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/l1-regularization":{"title":"What is l1 regularization?","content":"\nThe main objective of creating a model (training data) is making sure it fits the data properly and reduce the loss. Sometimes the model that is trained may fail and give a poor performance during analyzing the test data. This leads to overfittng. Regularization came to overcome overfitting.\n\nLasso regression (Least Absolute Shrinkage and Selection Operator) adds \"Absoule value of magnitude\" of coefficient, as penalty term to the loss function. Lasso shrinks the less important features coefficient to zero, thus removing some feature altogether. So this works well for feature selection in case we have a huge number of features.\n\n\nMethods like cross-validation, stepwise regression are there to handle overfitting and perform feature selection work well with a small set of features. There techniques are good when we are dealing with a large set of features.\n\nAlong with shrinking coefficients, the lasso performs feature selection as well. Because some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/l2-regularization":{"title":"What is l2 regularization?","content":"\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn't perform well on new/unseen data on which model wasn't trained on. To avoid overfitting, we do cross-validation sampling, reducing the number of features, pruning, regularization, etc.\n\nThe regressioin model that uses L2 regularization is called ridge regression. Regularization adds the penalty as model complexity increases. The regularization parameter ($\\lambda$) penalizes all the parameters except intercept so that the model generalized the data and won't overfit.\n\nRidge regression add \"squared magnitude of the coefficient\" as penalty term to the loss function. If the $\\lambda$ is zero, then it is equivalent to [OLS](ordinary-least-square-model.md). But if the lambda is very large, then it will add too much weight, and it will lead to underfitting.\n\nRidge regularization forces the weights to be small but doesn't not make them zero, and doesn't give the sparse solution. Ridge is not robust to outliers as square terms blow up the error differences of the outliers, and the regularization term tries to fix it by penalizing the weights.\n\nRidge regression performs better when all the input features influence the output, and all with weights are of roughly equal size. L2 regularization can learn complex data patterns.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/linear-regression":{"title":"What is linear regression?","content":"\n## Linear Regressions\n\nLinear regressions tends to establish a relationship between a dependent variable `Y` and one or more independent variable `X` by finding the best fit of the straight line.\n\nThe equation for linear model is $Y = mX + c$, where $m$ is the slope and $c$ is the intercept.\n\n![](images/linear-regression-1.png)\n\nIn the above diagram, the blue dots we see are the distribution of `y` with respect to `x`. There is no straight line that runs through all the data points. So, the objective here is to fit the best fit of a straight line that will try to minimize the error between the expected and actual value.\n\nRead more:\n- [What is ordinary least square model?](ordinary-least-square-model.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/logistic-regression":{"title":"logistic-regression","content":"\nThe logistic regression technique involves the dependent variable, which can be represented in the binary (0 or 1, true or false, yes or no) values, which means that **the outcome could only be in either one form of two**.\n\n","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/loss-function":{"title":"loss-function","content":"\n\u003e the cost that is incurred by the difference between the prediction and the true value. The larger the difference, the bigger the loss.","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/machine-learning":{"title":"What is machine learning?","content":"\nMachine learning is the scientific study of algorithms and statistical models that computer systems use to affectively perform a specific task **without using explicit instruction**, relyring on patterns and inference instead.\n\nBuilding a model by learning the patterns of historical data with some relationship between data to make a data-driven prediction.\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n# Training Data\n\n\u003e The relationship between the data and the machine learning model, is as critical as the fuel to the engine of rocket.\n\n\u003e It is not exaggerating to say that the data dictates how the machine learning model is built.\n\n## garbage in, garbage out\n\nGroup of blind men, who have never come across an elephant before, would like to learn and conceptualize what an elephant is like by touching it. \n\nEach man touches a part of the body, such as leg, tusk or tail _etc_. While each of them got a part of the reality, none of them has the whole picture of an elephant. Therefore, **none of them actually learned the true image of an elephant**.\n\n![](https://assets.leetcode.com/uploads/2018/12/16/card_elephant.png)\n\nThe training data we got could be those images of legs or tusks from an elephant, while during the test processing, the testing data we got are the full portraits of elephants. \n\nIt would not be surprising to find out that our trained model does not perform well in this case, since we do not have the _**high-quality**_ training data that is closer to the reality in the first place.\n\nIf the data is really important, then why not feeding the \"high-quality\" data such as full portraits of elephants into the algorithm, instead of snapshots on parts of the elephant body?\n- Because, facing a problem, we or the machine, like the \"blind-men\", **often struggle to gather the data** that captures the essential characteristics of the problem, either due to the technical issues (_e.g._ data privacy) or simply because we do not perceive the problem in the right way.\n\n## Real world data\n\nIn the real world, the data we got reflects a part of reality in a favorable case, or it could be some noise in a less favorable case, or in the worst case, even a contradiction to the reality. \n\nRegardless of the machine learning algorithms, one would not be able to learn anything from data that contains too much noise or is too inconsistent with the reality.\n\n# Workflow\n\n\u003e The relationship between the data and the machine learning model, is as critical as the fuel to the engine of rocket.\n\n## Data-Centric Workflow\n\n![](https://assets.leetcode.com/uploads/2018/11/25/ml_workflow.pngthe outermost interface that we interact with the model)\n\n# Reference\n\n- https://assets.leetcode.com/uploads/2018/12/16/card_elephant.png","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/machine-learning-algorithm":{"title":"machine-learning-algorithm","content":"\nA machine learning algorithm is the process that uncovers the underlying relationship whith the data\n\nThe outcome of a machine learning algorithm is called [[machine-learning-model]], which can be considered as a `function` $F$, which outputs certain results, when given the input.\n\nReference:\n- https://leetcode.com/explore/featured/card/machine-learning-101/287/what_is_ml/1617/","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/machine-learning-model":{"title":"machine-learning-model","content":"\nRather than a predifined and fixed function, a machine learning model is dedrived from historical data. Therefore, when fed with with different data, the output of [[machine-learning-algorithm]] changes, i.e the machine learning model changes.\n\nReference:\n- https://leetcode.com/explore/featured/card/machine-learning-101/287/what_is_ml/1617/","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/main-prediction":{"title":"main-prediction","content":"\n\u003e a learner is playing a dart-throwing game\n\n\u003e _how many points that the player would score?_\n\nhttps://leetcode.com/explore/featured/card/machine-learning-101/281/how_to_ml/2642/\n\n","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/mean-absolute-error":{"title":"mean-absolute-error","content":"\n## Interpretation\n\n\u003e On average, our predictions are off by about X","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/mean-square-error":{"title":"What is mean square error?","content":"\nThe mean squared error tells you how close a regression line is to a set of points. It does this by taking the distance from the points to the regression line (these distance are the \"errors\") and squaring them.\n\nGiving an intuition\n\n![](images/mean-square-error-1.png)\n\nThe line equation in `Y = mX + c`. We want to find `m` (slope) and `c` (intercept) that minimize the squared error.\n\n$$\nMSE = \\frac{1}{n} \\displaystyle\\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2\n$$\n\nRead more: [Linear Regression](linear-regression.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/optimizers":{"title":"optimizers","content":"\n\n","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/ordinary-least-square-model":{"title":"What is ordinary least square model?","content":"\nOLS (Ordinary Least Square) is a stats model, which will help us in identifying the more significant features that can has an influence on the output.\n\nOLS model in python is executed as:\n\n```python\nlm = smf.ols(formula='Sales ~ am + constant', data=data).fit()\nlm.conf_int()\nlm.summary()\n```\n\nand we got the output as below,\n\n![](images/ordinary-least-square-model-1.png)\n\nThe higher the `t-value` for the feature, the more significant the feature is to the output variable. The `p-value` plays a rule in rejecting the null hypothesis. If the `p-value` is less than `0.05` (95% confidence interval) for a feature, then we can consider the feature to be significant.","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/overfitting":{"title":"overfitting","content":"\n\u003e An overfitting model is the one that fits well with the training data, _i.e._ little or no error, however it does not generalized well to the unseen data\n\n\u003e a model matches the training data almost perfectly, but does poorly in validation and other new data\n\n\u003e overfitting capturing spurious (false) patterns that won't recur in the future, leading to less accurate predictions\n\nHow to avoid overfitting?\n- try out another algorithm that could **generate a simpler model** from the training data set\n- adds a [[regularization]] term to the algorithm, _i.e._ penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data\n","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/p-value":{"title":"p-value","content":"\nWhat is p-value?\n- p-value is a statistical measurement used to validate a hypothesis againts observed data\n\nWhat does the p-value measures?\n- p-value measure the probability of obtaining the observed results, assuming that the null hypothesis is true\n\nHow to calculate p-value?\n1. Find out the test statistic $z$ \n\t- $z = \\frac{\\hat p - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}$\n\t- where:\n\t  $\\hat p$ = sample proportion\n\t  $p_0$ = assumed population proportion in the null hypothesis\n\t  $n$ = sample size \n2. Look at the z-table to find the corresponding level of p from the z value obtained ","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/positioning":{"title":"positioning","content":"\nPositioning helps you understand **how to talk about your product** / service, in order to maximize customer lifetime value\n\n\u003e What type of brand message will increase and ensure brand trust?\n\n\u003e What type of brand message is likely to induce a purchase interaction?","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/probability-and-statistically-significance-explained":{"title":"probability-and-statistically-significance-explained","content":"\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/09/p-value-768x562.png)\n\n## Significance level (alpha)\n\n- Denoted as alpha or $\\alpha$\n- The probability of rejecting the null hypothesis when it is true\n- Generally, we use the significance value of 0.05\n\n\n## P-value\n\n- The probability that the difference between two values is just because random chance\n- P-value is evidance againts the null hypothesis\n- The smaller p-value stronger the chances to reject the $H_0$\n- For the significance level of 0.05, if the pvalue is lesser than it, hence we can reject the null hypothesis\n\n## Confidence Interval\n\n- The confidence interval is an observed range in which a given percentage of test outcomes fall\n- Generally, we take a 95% confidence interval","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/project-audio-conversation-transcription":{"title":"project-audio-conversation-transcription","content":"\n## Details\n\nPackages:\n- TensorFlow: Model\n- SoundFile:\n- Pydub\n- webrtcvad\n- pyannote.audio\n- scikit-learn: hierarchical, spectral, silhouette_score\n- scipy: read and write audio files, create laplacian matrix spectral clustering\n\n## Questions\n\nBagaimana model deep learningnya?\n- Shared CNN + VLAD layer\n- [[vgg-speaker-recognition]]\n\nBagaimana cara kerja voice activity detectionnya?\n- https://github.com/wiseman/py-webrtcvad\n- WebRTC: write in C\n\nMetrics apa yang digunakan?\n- [[diarization-error-rate]]\n- ketepatan clustering\n\nBagaimana membuat datasetnya?\n- dataset percakapan yang dibentuk dari potongan-potongan suara voxceleb1\n\nApa perbedaan hierarchical clustering dan spectral clustering?\n- ","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/project-madani":{"title":"project-madani","content":"\n","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/r-squared":{"title":"What is R squared?","content":"\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. \n\nR-squared explained `variation / total variation`. R-squared is always between 0% and 100%. 0% indicates that the model explains none of the variability of the response data around it's mean. 100% indicates that the model explains all the variability of the response data around it's mean. In general, the higher the R-squared, the better the model fits your data.\n\n$$\nR^2 = 1 - \\frac{SS_{Regression}}{SS_{Total}} = \\frac{\\sum_i{(y_i - \\hat{y_i})^2}}{\\sum_i{(y_i - \\bar{y_i})^2}}\n$$\n\nThere is a problem with the R-squared. The problem araises when we ask this question to ourselves. \"Is it good to help as many independent variable as possible?\" The answer is No, because we understood that each independent variable should have a meaningful impact. \n\nBut, even if we add independent variables which are not meaningfully. will it improve R-squared value? Yes, this is the basic problem with R-squared. How many junk independent variables or important independent variable or impactful independent variable you add to your model, **the R-squared value will always increase**. It will never decrease with the addition of a newly independent variable, whether it could be an impactful, non-impactful, or bad variable. So we need another way to measure equivalent R-squared, which panilizes our model with any junk independent variable.\n\nSo, we calculate the Adjusted R-squared with a better adjustment in the formula of generic R-squared.\n\n$$\nR^2 adjusted = 1 - \\frac{(1-R^2)(N-1)}{N-p-1}\n$$\n\nwhere, $R^2$ is the sample R-square, $p$ is the number of predictors, and $N$ is the total sample size.","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/random-forest":{"title":"random-forest","content":"\n# Advantages \u0026 Disadvantages\n\n## Advantages\n\n- Reduces [[overfitting]]\n- Higher accuracy compared to other models\n\n\u003e Random Forests address both [[decision-tree]] issues ([[overfitting]] and instability). The idea is to **construct multiple trees using different subsets of the training data and features** for each tree in the forest. Then, **aggregate** their predictions by outputting the majority vote or the average value\n\nThe rationale is that **an ensemble of models is likely more accurate than a single tree**. So, even if a tree overfits its subset, we expect other trees in the forest to compensate for it. That’s why we use different subsets for each tree, to force them to approach the problem from different angles.\n\nThe random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree.\n\n## Disadvantages\n\n- Training complexity can be high\n- Not very interpretable\n\n\u003e **A single tree is interpretable, whereas a forest is not**\n\nHumans can visualize and understand a tree, no matter if they’re machine learning experts or laypeople. That’s not the case with forests. They contain a lot of trees, so **explaining how they output the aggregated predictions is very hard,** if not impossible.\n\n# Reference\n- https://www.baeldung.com/cs/decision-trees-vs-random-forests\n- https://www.kaggle.com/code/dansbecker/random-forests/tutorial\n","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/random-sampling":{"title":"random-sampling","content":"\n\u003e It's a technique where each sample in a population has an equal change of being chosen\n\nIt eliminates sampling bias -\u003e We want our sample to be representative of the entire population rather the sample itself","lastmodified":"2022-12-02T14:20:10.464295932Z","tags":null},"/random-state":{"title":"random-state","content":"\nMany machine learning models allow some randomness in model training. Specifying a number for `random_state` ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won't depend meaningfully on exactly what value you choose.","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/regularization":{"title":"regularization","content":"\n\u003e penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data\n\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/rfm-analysis":{"title":"rfm-analysis","content":"\n## What is RFM?\n\n\u003e Are all my customers similar?\n\n\u003e What differentiated them from each other?\n\n\u003e Who is the most likely customer?\n\n\u003e Who are my best customers?\n\n\u003e Which customer has the potential to buy more?\n\n\u003e Which customer has been churned out / has lapsed?\n\n\u003e Which customer can be converted by creating value through promotions?\n\n\u003e Which customer is likely to be loyal in the near future?\n\n\u003e What brand means the existing customers?\n\nSummary:\n- Built on historical transactions between user and the business\n- Uses R, F, and M variables of customer data\n- Analyses the **entire population**\n- No need to create curated sample sets\n- Dependent on efficient and accurate data\n- No scope for human error\n\nGoals:\n- Help businesses differentiate between marketing to existing and new customers\n- Helps them create relevant and personalized messaging by understanding user behavior\n- Help businesses magane customer perceptions\n- Segment it's customers based on three criteria, based on an existing customer's transaction history\n- Prevent churn by using fundamental marketing principles of segmentation, [[targeting]], and [[positioning]]\n- Translate positive sentiment into purchase opportunities\n- Allow us to devide potential customer groups, allowing business to **talk to them separately**\n\nPareto principle to RFM:\n-  80% of total results are driven by the top 20% causes\n- In marketing: 80% of your total sales are likely to come from your top 20% of customers\n\n\nWhy customer segmentation is highly critical\n- Regular customers will always be high contributors to business monetary value, and hence that customer \n\n\nAll R, F, M criteria can be graded on a scale of 1 to 5. It is also critical to specify an appropriate range for each grade, in order to create a customer group with a similar or a particular behavior\n\n### Recency\n\n\u003e How recenly the user interacted with the website/app?\n\n\u003e When was the last time your cusotmer purchased a product/service?\n\nExample: Days since last purchase/visit\n\nInterpretation:\n- High recency: Customer has positively considered your brand for a purchase decision recently\n\nHow to calculate:\n- Can be scored by grading on custom-built filters such as bought on the last days / 1 month / 3 months and so on, depending on the nature of the business\n\n### Frequency\n\n\u003e How frequently the user interact\n\nExample: Total number of days when a purchase/visitt was done\n\nInterpretation:\n- High frequency: Customer buys your brand frequently and id likely to be a **loyalist of your brand**\n\nHow to calculate:\n- Business need to analyze the total number of purchases completed by customers in a fixed time period\n- Scored by grading on custom-built filters such as bought thrice in a year / bought once a month and so on, depending on the nature of the business\n\n### Monetary\n\n\u003e How much do they spend?\n\nExample: Customer lifetime value\n\nInterpretation:\n- High monetary value scores: A customer is the highest purchase history of your brand \n\n## Advantages\n\n\nWhy is RFM analysis is better than traditional segmentation model?\n\n\u003e The RFM analysis **built on transactions between the customer and the business**, to create a robust data-backend method based on hard numbers \n\nThis customer data is graded, analyzed, and then segmented in order to engage customers as distinct groups. \n\nThis model helps businesses effectively **analyze the past buying behavior of each customer**, to predict and shape future customer behavior\n\n## Difference between RFM and traditional segmentation methods\n\n![](https://cdn-clalk.nitrocdn.com/KqmKVeLhgFAzHWrUbBzmAbRgoFMrOqoq/assets/static/optimized/rev-2cab980/wp-content/uploads/2020/07/RFM-vs-traditional-segmentation-e1579855251421-1-1024x458.jpg)\n\n\nTraditional methods:\n- Traditional methods of customer segmentation, used by market research companies before the advent of data analytics, used variables like **demographic and psychographic factors** to group their customers\n- A sample could be incorrect, due to many reasons like an insufficient number of consumers, incorrect gender balance, varying psychigraphic factors, etc\n- Traditional research involved factors like psychographics, which could be interpreted subjectively\n\n\n\n\n\n\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/semi-supervised-learning":{"title":"semi-supervised-learning","content":"\nThe data set is massive but the labeled sample are few\n\nTraining strategy:\n- we train a model with the labeled data, then we apply the model to predict the unlabeled data\n- first cluster the images into groups (unsupervised learning), and then apply the supervised learning algorithm on each of the groups individually\n\n\n\n# Reference","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/sigmoid":{"title":"sigmoid","content":"\nf(x) = 1 / (1+exp(-x))\n\nIt's range between 0 and 1\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/standard-deviation":{"title":"standard-deviation","content":"\n\u003e standard deviation measures how numerically spread out the values are","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/stochastic-gradient-descent":{"title":"stochastic gradient descent","content":"\nAlgorithm:\n- Shuffle the datasets -\u003e so that we get a randomised dataset\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/supervised-learning":{"title":"supervised-learning","content":"\n\u003e An important measurement for supervised learning algorithms, is the _**generalization**_, which measures how well that a model derived from the training data can predict the desired attribute of the unseen data.\n\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/support-vector-regression":{"title":"What is support vector regression?","content":"\nWhy SVR (support vector regression)? What is the difference between SVR and a simple regression model?\n\nThe answer is, simple linear regression try to minimize the error rate. But in SVR, we try to fit the error within a certain threshold.\n\n![](images/support-vector-regression-1.png)\n\nMain concept:\n- Boundary Line (Red Line)\n- Kernel\n- Support Vector\n- Hyperplane (Blue Line)\n\nOur best fit line is the one where the hyperplane has the maximum number of points. What we are trying to do here is to decide a decision boundary at \"e\" distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are whithin that boundary line.\n\n\n![](images/support-vector-regression-2.png)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/t-test":{"title":"t-test","content":"\n## Two-sample T-test\n\n\u003e Most commonly used hypothesis test. It is **applied to compare whether the average difference between the two groups**\n\n\n## Formula\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/09/test_sta.png)","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/tags-list":{"title":"tags-list","content":"\n- all-post\n- algorithm\n- what-is","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/tags-system":{"title":"Tags System","content":"\n# PARA Methods\n\nP.A.R.A. stands for **Projects — Areas — Resources — Archives**, the four top-level categories that encompass every type of information you might encounter in your work and life.\n\n- Projects: a series of tasks linked to a goal\n- Area: a sphere of activity with a standard to be maintained over time\n- Resource: a topic or theme of ongoing interest\n- Archive: inactive items from the other three categories\n\nexample tags:\n- project-project_name\n- area-ds\n\nResource:\n- [fortelabs](https://fortelabs.com/blog/para/)","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/tags/authors-ali-abdaal":{"title":"","content":"","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/tanh":{"title":"tanh","content":"\nTanh also called a hyperbolic tangent function\n\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/targeting":{"title":"targeting","content":"\nTargteting involves:\n- understanding the **routines** and **customer behavior** of these segments, allowing you to consider and choose the ideal way to speak to them\n\n\u003e Where do my customer interact with the brand?\n\n\u003e What's the best time, place, medium, and format to talk to them about my brand?\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/time-series":{"title":"time series","content":"\nTest\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/uji-rata-rata-1-populasi":{"title":"uji-rata-rata-1-populasi","content":"\nBagaimana melakukan Uji Rata-rata 1 Populasi dengan nilai varians diketahui?\n- Diasumsikan bahwa $X \\sim N(\\mu, \\sigma^2) ; \\sigma^2\u003e0$\n- Hipotesis uji satu arah \n    - $H_0:\\mu=\\mu_0$\n    - $H_1:\\mu\u003e\\mu_0$ atau $H_1:\\mu\u003c\\mu_0$ (uji sisi kanan dan sisi kiri)\n- Hipotesis uji dua arah \n    - $H_0:\\mu=\\mu_0$\n     - $H_1:\\mu \\neq \\mu_0$\n- Statistik uji\n\t- $Z_{hit} = \\frac{\\bar X - \\mu_0}{\\sigma / \\sqrt{n}}$\n- Daerah kritis / penolakan $H_0$ uji satu arah \n\t- Tolak $H_0$  jika $Z_{hit} \u003e Z_{\\alpha}$ (sisi kanan)\n\t- Tolak $H_0$  jika $Z_{hit} \u003c -Z_{\\alpha}$ (sisi kiri)\n    Daerah kritis / penolakan H_0 uji dua arah \n        Tolak $Z_{hit} \u003e Z_{\\alpha/2}$ jika $Z_{hit}$ \u003c $-Z_{\\alpha/2}$ atau jika $Z_{hit} \u003c -Z_{\\alpha}$ \n\nBagaimana melakukan Uji Rata-rata 1 Populasi dengan nilai varians tidak diketahui?↓↓\n- Diasumsikan bahwa $X \\sim N(\\mu, \\sigma^2) ; \\sigma^2\u003e0$\n- Hipotesis uji satu arah\n\t- $H_0:\\mu=\\mu_0$ \n\t- $H_1:\\mu\u003e\\mu_0$ atau $H_1:\\mu\u003c\\mu_0$ (uji sisi kanan dan sisi kiri)\n- Hipotesis uji dua arah\n\t- $H_0:\\mu=\\mu_0$\n\t- $H_1:\\mu \\neq \\mu_0$\n- Statistik uji\n\t- $T_{hit} = \\frac{\\bar X - \\mu_0}{s / \\sqrt{n}}$\n- Daerah kritis / penolakan $H_0$ uji satu arah\n\t- Tolak $H_0$ jika $Z_{hit} \u003e Z_{\\alpha}$ (sisi kanan)\n\t- Tolak $H_0$ jika $Z_{hit} \u003c -Z_{\\alpha}$ (sisi kiri)\n- Daerah kritis / penolakan $H_0$ uji dua arah\n\t- Tolak $Z_{hit} \u003e Z_{\\alpha/2}$ jika $Z_{hit} \u003c -Z_{\\alpha/2}$ atau jika $Z_{hit} \u003c -Z_{\\alpha}$","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/under-coverage-bias":{"title":"under-coverage-bias","content":"\n\u003e It is the bias from sampling too few observations","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/underfitting":{"title":"underfitting","content":"\n\u003e An underfitting model is the one that does not fit well with the training data, _i.e._ significantly deviated from the ground truth\n\n\u003e Underfitting **failing to capture relevant patterns**, again leading to less accurate predictions\n\nCauses:\n- the model is over-simplified for the data, therefore it is not capable to capture the hidden relationship within the data\n\n![](https://assets.leetcode.com/uploads/2019/01/01/underfitting.png)\n\n\u003e Read also: [[underfitting-overfitting]]\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/underfitting-overfitting":{"title":"underfitting-overfitting","content":"\n\u003e When we say a model is underfitting or overfitting, it implies that the model does not generalized well to the unseen data.\n\nwhy a model that fits well with the training data does not necessarily imply that it would generalize well to the unseen data? because...\n- **the training data are just samples we collect from the real world**, which represents only a proportion of reality. It could be the case that the training data is simply not representative, thus even the model fits perfectly the training data, it would not fit well with the unseen data\n- **the data that we collect contains noises and errors inevitably**. The model that fits perfectly with the data, would also capture the undesired noises and errors by mistake, which would eventually lead to bias and errors in the prediction for the unseen data\n\n![](https://assets.leetcode.com/uploads/2019/01/01/underfitting.png)\n\n\u003e Read also: [[underfitting]]","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/upsampling-downsampling":{"title":"upsampling and downsampling","content":"\nDegree of imbalance:\n- Mid: 20-40%\n- Moderate: 1-20%\n- Extreme: 1%\n\nUp-sampling\n- Randomly duplicating observations from minority class\n- Tools: `sklearn.utils.resample`\n\nDown-sampling\n- Randomly removing observations from the majority class\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/variance":{"title":"variance","content":"\n\u003e variance is the tendency to learn random things unrelated to the real signal\n\n\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/vim-bindings":{"title":"vim-bindings","content":"\n- move around: h, j, k, l\n- go to the start / end of a line: 0, $\n- move between words: w, e, b\n- find the next/before the same word: *, #\n- move to to the beginning of the line and back to end: gg, G\n- go to line 2: 2G\n- search text: / text\n\t- n next search\n\t- N previous search\n- create new line after / before: o, O\n- detele character under cursor and to the left of the cursor: x, X\n- delete line: dd\n- select multiple letter to delete (using visual mode): v, e, d\n- undo: u\n- redo: crtl + r\n- help: :help\n- save: :w\n- quit: :q\n- quit without saving: :q!\n\n\ntsett tset \n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null},"/weights":{"title":"weights in deep learning","content":"\nWeights are usually initialised randomly. The initialization takes a fair amount of repetitions to converge to the least loss and reach the ideal weight matrix.\n\nThe problem is that kind of initialization is prone to vanishing or [[exploding-gradient]] problesms\n\nGeneral ways to make it initialize better weight:\nUsing [[relu]] activation function in the deep nets\n- generate random sample of weights from a [[gaussian-distribution]] having mean 0 and standard deviation 1\n- multiply the sample with the square root of (2/num of input unit for that layer)\n\nUsing [[tanh]] activation function\n- generate random sample of weights from a [[gaussian-distribution]] having mean 0 and standard deviation 1\n- multiply the sample with the square root of (1/several input unit for that layer)\n","lastmodified":"2022-12-02T14:20:10.468296206Z","tags":null}}