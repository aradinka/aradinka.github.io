{"/":{"title":"Aradinka | Digital Garden","content":"\nThis website meant to be my digital garden built using [Quartz](https://github.com/jackyzha0/quartz).\n\nCheck out my portfolio **[aradinka.com](https://aradinka.com)**\n\n[All Post](/tags/all-post)","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/_areas":{"title":"_areas","content":"\n[Productivity](/tags/areas-productivity)","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/_authors":{"title":"_authors","content":"\n[Ali Abdaal](/tags/authors-ali-abdaal)","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/ab-testing":{"title":"ab-testing","content":"\n## Overview\n\n\u003e A/B testing is a popular way to test your products. A/B testing is one of the most prominent and widely used statistical tools.\n\n## Example Scenario\n\n\u003e  You have made certain changes to your website recently. Unfortunately, **you have no way of knowing with full accuracy how the next 100,000 people who visit your website will behave**. That is the information we cannot know today, and if we were to wait until those 100,000 people visited our site, it would be too late to optimize their experience.\n\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/09/ab_test.png)\n\nHere A will remain unchanged while you make significant changes in B’s packaging. Now, on the basis of the response from customer groups who used A and B respectively, you try to decide which is performing better.\n\n## What is A/B testing?\n\nA/B testing is a basic **randomized control experiment**. It is a way to compare the two version of a variable to find out which performs better **in a controlled environment**.\n\nIt's a hypothetical testing methodology for makin decisions that estimate population parameters based on sample statistics.\n- Population: All customers buying the product\n- Sample: The number of customers that participated the test\n\n## Steps\n\n### 1. Make a Hypothesis\n\n\u003e A hypothesis is a tentative insight into the natural world\n\nA [[hypothesis-testing]] is a concept that is not yet verified, but if true would explain certain facts or phenomena.\n\nIn this example, the hypothesis can be \"By making changes in the design of the new page in the website, we can get more traffic on the website\"\n\n\nNull hypothesis $H_0:$\n- $H_0:$ There is no difference in the conversion rate in customer receiving pages A and B\n\nAlternative hypothesis $H_1:$\n- $H_1:$ The conversion rate of pages B is higher than those who receive newsletter A\n\n### 2. Create Control Group and Test Group\n\n\u003e Dedcide the group of customers that wil participate in the test.\n\nThe control group is the one that will receive newsletter A and the test group is the one will receive newsletter B.\n\nFor an experiment, we randomly select 1000 cutomers using [[random-sampling]]. 500 each for control group and testt group.\n\n**We must take care of the sample size**. It's required that we determine the minimum sample size for our A/B testing before confucting it so that we can eliminate [[under-coverage-bias]]. It's the bias from smapling too few observations.\n\n### 3. Conduct A/B Test and Collect the Data\n\nOne way to perform the test is to calculate daily [[conversion-rate]] for both the treatment and the control groups. Since the [[conversion-rate]] in a group on a certain day represents a single datat point, **the sample size is actually the number of days**. Thus, **we will be testing the difference between the mean of daily conversion rates in each group** across the testing period.\n\nWhen we run our experiment for one month, we noticed that the mean [[conversion-rate]] for the control group is **16%** whereas that for the test group is **19%**.\n\n\u003e Can we conclude from here that the Test group is working better than the control group? -\u003e Use statistical test\n\n## Statistical Significance of the Test\n\nFor rejecting out null hypothesis we have to prove the **statistical significance** of our test.\n\n\n\n## Code\n\n### Check whether 2 groups are similar or not\n\nSteps:\n- Split and define control \u0026 test group\n- Check normality: Shapiro test (pval \u003e 0.05 = normal)\n- If group A \u0026 B normal -\u003e check homogeneity of variance using levene test (pval \u003e 0.05 = homogen) -\u003e ttest (H0: M1 == M2)\n- If one of A / B not normal -\u003e non-parametric mann whitney u test\n\nCode sample\n```python\nfrom scipy.stats import shapiro\nimport scipy.stats as stats\n\ndf[\"version\"] = np.where(df.version == \"gate_30\", \"A\", \"B\")\n\ngroup = \"version\"\ntarget = \"sum_gamerounds\"\n\ngroupA = df[df[group] == \"A\"][target]\ngroupB = df[df[group] == \"B\"][target]\n\n# Normality Assumption, \n# H0: Fulfilled\n# H1: Not Fultilled\n\nntA = shapiro(groupA)[1] \u003c 0.05\nntB = shapiro(groupB)[1] \u003c 0.05\n\nif (ntA == False) \u0026 (ntB == False): # Normality Fulfilled, # Parametric Test\n    # Assumption: Homogeneity of Variance\n    # H0: homogeinety: False\n    # H1: Heterogenerous: True\n    leveneTest = stats.levene(groupA, groupB)[1] \u003c 0.05\n    \n    if leveneTest == False:\n        # Homogeneity\n        # H0: M1 == M2: False\n        # H1: M1 != M2: True\n        ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n    else:\n        # Heterogenous\n        # H0: M1 == M2: False\n        # H1: M1 != M2: True\n        ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n\nelse: # Normality Violated, # Non-Parametric Test\n    # H0: M1 == M2: False\n    # H1: M1 != M2: True\n    ttest = stats.mannwhitneyu(groupA, groupB)[1]\n\nres = pd.DataFrame({\n    \"AB Hypothesis\": [ttest \u003c 0.05],\n    \"pvalue\": [ttest]\n})\n\nres[\"Test Type\"] = np.where((ntA == False) \u0026 (ntB == False), \"Parametric\", \"Non-Parametric\")\nres[\"AB Hypothesis\"] = np.where(res[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\nres[\"Comment\"] = np.where(res[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar\", \"A/B groups are not similar\")\n\nif (ntA == False) \u0026 (ntB == False):\n    res[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n    res = res[[\"Test Type\", \"Homogeneity\", \"AB Hypothesis\", \"pvalue\", \"Comment\"]]\nelse:\n    res = res[[\"Test Type\", \"AB Hypothesis\", \"pvalue\", \"Comment\"]]\nres\n```\n\n\n\n","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/analysis-step":{"title":"analysis-step","content":"\n- check `.columns`\n- `.describe()` data\n\t- check missing values\n\t- check mean \u0026 [[standard-deviation]] values\n\t- check min, percentil, max valeus\n- ","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/batch-size":{"title":"batch-size","content":"\nBaca tentang [[batch-nomalization]]\n\nBatch size salah satu yang bisa kita tuning. Tuning batch sizenya sampai kita mendapatkan performa yang maksimum \n\nPengaruh batch size pada performa model:\n- Terlalu kecil: Distribusi hasil batch normalization kurang bagus. Kadang skew ke kiri kadang ke kanan\n- Terlalu besar: Hessian matrix tidak mencerminkan local minima\n\n- Apabila menggunakan batch size besar, gunakan [[learning-rate]] yang kecil\n\n\n[Innovation Day: MultiGPU Infrastructure \u0026 Implementation](https://youtu.be/IayDLHyHqlE?t=3574)","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/bias":{"title":"bias","content":"\n\u003e Bias is a learner’s tendency to consistently learn the same wrong thing","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/bias-vs-variance":{"title":"bias-vs-variance","content":"\nIt provides another perspective to look at the phenomenon of [[underfitting]] and [[overfitting]]\n\n\u003e [[bias]] is a learner’s tendency to consistently learn the same wrong thing\n\n\u003e [[variance]] is the tendency to learn random things unrelated to the real signal\n\n","lastmodified":"2022-11-26T16:30:16.85796109Z","tags":null},"/cheat-sheet":{"title":"cheat-sheet","content":"\n\n# ML Algorithm\n\n## Datacamp\n\n![](cheat-sheet-ml-datacamp.jpg)\n\nhttps://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet\n\n## Microsoft Azure\n\n![](cheat-sheet-ml-microsoft-azure.png)\n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/algorithm-cheat-sheet","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/classification":{"title":"classification","content":"\n# Why we can’t do a classification problem using Regression?\n\nWith linear regression you fit a polynomial through the data - say, like on the example below, we fit a straight line through {tumor size, tumor type} sample set:\n\n![](images/classification-1.png)\n\nAbove, malignant tumors get 1, and non-malignant ones get 0, and the green line is our hypothesis $h(x)$. To make predictions, we may say that for any given tumor size x, if $h(x)$ gets bigger than 0.5, we predict malignant tumors. Otherwise, we predict benignly.\n\nIt looks like this way, we could correctly predict every single training set sample, but now let's change the task a bit.\n\n![](images/classification-2.png)\n\nNow our $h(x)\u003e0.5$→malignant doesn't work anymore. To keep making correct predictions, we need to change it to $h(x)\u003e0.2$ or something - but that not how the algorithm should work.\n\n**We cannot change the hypothesis each time a new sample arrives**. Instead, we should learn it off the training set data, and then (using the hypothesis we've learned) make correct predictions for the data we haven't seen before.\n\n\u003e **Linear regression is unbounded**\n\n\u003e Futher reading: [[linear-regression]], [[logistic-regression]]","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/code-screen":{"title":"code-screen","content":"\n# outside session\n\n```\n# create screen session\nscreen -S session_name\n\n# kill complete session\nscreen -S [session_name] -X quit\n\n# detatch running session\nscreen -d [session_name}]\n```\n\n# inside session\n\n```\n# create new window\nctrl a, c\n\n# list all window\nctrl a, \"\n\n# split horizontally\nctrl a, s\n\n# change between tab\nctrl a, tab\n\n# exit window\nctrl a, d\n```\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/code-sklearn":{"title":"code-sklearn","content":"\n```\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_absolute_error\n```","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/code-sql":{"title":"code-sql","content":"\n## Manipulation\n\n### Create a table with column constraints\n\n```sql\nCREATE TABLE table_name (\n\t-- Uniquely identify the row\n\tid INTEGER PRIMARY KEY,\n\t\n\t-- Columns have a different value for every row \n\tname TEXT UNIQUE\n\t\n\t-- Columns must have a value\n\tgrade INTEGER NOT NULL\n\t\n\t-- Assigns a default value when no value is specified\n\tage INTEGER DEFAULT 10\n);\n```\n\n### `INSERT` statement\n\n```sql\n-- Insert into columns in order\nINSERT INTO table_name\nVALUES (value1, value2);\n\n-- Insert into columns by name\nINSERT INTO table_name (column1, column2)\nVALUES (value1, value2);\n```\n\n### `ALTER TABLE` statement\n\n```sql\n-- Modify the columns of an existing table\n-- When combined with ADD clause, it used to add a new column\nALTER TABLE table_name\nADD column_name datatype;\n```\n\n### `DELETE` statement\n\n```sql\n-- Delete records (rows) in table\n-- WHERE clause specifies which records that should be deleted\n-- If WHERE clause omitted, all records will be deleted\nDELETE FROM table_name\nWHERE some_column = some_value;\n```\n\n### `UPDATE` statement\n\n```sql\n-- Edit records (rows) in table\n-- It includes a SET clause that indicate the column to edit\n-- and a WHERE clause for specifying the records\nUPDATE table_name\nSET column1 = value1, column2 = value2\nWHERE some_column = some_value;\n```\n\n\n## Reference\n\n- https://www.codecademy.com/learn","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/conversion-rate":{"title":"conversion-rate","content":"\n## What is conversion rate?\n\n\u003e Conversion rate is often used by e-commerce sites to measure the **percentage of visitors that end up purchasing products**.\n\n\n### Formula\n\n**Conversion rate = (Conversions or goals achieved / Total visitors) * 100**\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/convolutional-neural-network":{"title":"convolutional-neural-network","content":"\nKenapa cnn bagus untuk dipakai di data image, text, audio dan video?\n- Mencerminkan spatial correlation between features\n- Di data image: Pixel di titik `i, j` sangat berkaitan dengan `i+1, j+1`. Dan sekitarnya1\n- Di data text: Text sebelum dan sesudah saling berhubungan\n\n[](https://youtu.be/IayDLHyHqlE?t=3725)","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/decision-tree":{"title":"decision-tree","content":"\n## What is decision tree?\n\n\u003e A decision tree is a tree-shaped model guiding us in **which order to check the features of an object**, to **output its discrete or continuous label**.\n\n\u003e a type of supervised learning algorithm that **can be used in classification as well as regressor problems**\n\n\u003e works on an if-then statement, tries to solve a problem by using tree representation (node and leaf)\n\nInput: can be both continuous as well as categorical\n\nAssumptions while creating a decision tree:\n1. Initially all the training set is considered as a root\n2. **Feature values are preferred to be categorical**, if continuous then they are discretized\n3. Records are distributed recursively on the basis of attribute values\n4. Which attributes are considered to be in root node or internal node is done by using a statistical approach\n\n![](images/decision-tree-1.png)\n\n---\n\n\u003e The internal nodes tell us which features to check, and the leaves reveal the tree’s prediction\n\n\n![](https://www.baeldung.com/wp-content/uploads/sites/4/2022/03/decision_tree1.jpg)\n\n- Each leaf contains a subset of the training dataset\n- All its instances pass all the checks on the path from the root to the leaf\n- When predicting the outcome for a new object, we assign it the combined label of the training data that end in the same leaf as the instance. In the classification problems, it’s the associated subset’s majority class. Similarly, it’s the average value in regression\n\n\n## Use Cases\n\n- [[customer-churn-prediction]]\n- [[credit-score-modelling]]\n- [[disease-prediction]]\n\n## Advantages \u0026 Disadvantages\n\nAdvantages:\n- explainable and interpretable\n- can handle missing values\n\nDisadvantages:\n- Prone to [[overfitting]]\n- Sensitive to [[outlier]]\n\n### Overfitting and Instability of Decision Trees\n\n\nOverfitting:\n- Decision trees prone to [[overfitting]] the data. Since **accuracy improves with each internal node**, **training will tend to grow a tree to its maximum** to improve the performance metrics.\n- That deteriorates the tree’s generalization capability and usefulness on unseen data since it will start modeling the noise\n\nInstability:\n- We can limit the tree’s depth beforehand, but there’s still the problem of instability\n- even **small changes to the training data**, such as excluding a few instances, **can result in a completely different tree**\n\n## Depth of the tree\n\nWhen we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are quite close to those homes' actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).\n\nOn the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups. \n\nAt an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason)\n\n A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n\n\n## Splitting the Data\n\n\u003e When splitting, we choose to partition the data by the attribute that results in the **smallest impurity** of the new nodes\n\n\n## Control overfitting in decision tree\n\n_max_leaf_nodes_ argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area to the overfitting area.\n\n## Algorithm\n\n\u003e A decision tree uses different algorithms to decide whether to split a node into two or more sub-nodes. **The algorithm chooses the partition maximizing the purity of the split (i.e., minimizing the [[impurity]])**\n\nimpurity is a measure of **homogeneity** of the labels at the node at hand\n\n### ID3 (Iterative Dichotomiser)\n\n- Uses [[entropy]] and [[information-gain]] as metrics to form a better decision tree\n- The attribute with the highest information gain is used as a root node, and a similar approach is followed after that \n- Entropy is the measure that characterizes the [[impurity]] of an arbitrary collection of examples\n- when entropy $H(S)=0$, the set is perfectly classified (all element in $S$ are of the same class)\n- when entropy $H(S)=1$, the class distribution is equal\n- entropy is calculated for each remaining attribute, the attribute with the smallest entropy is used to split the set $S$ on this iteration\n- the higher the entropy, the higher potential to improve the classification here\n\n![](entropy-1.png)\n\n\n## Reference\n\n- https://www.baeldung.com/cs/decision-trees-vs-random-forests","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/difference-between-ai-ds-ml-dl":{"title":"What is the difference between AI, DS, ML and DL?","content":"\n## Artificial Intelligence\n\n**AI is purely math and scientifc exercise**, but when it became computational, it started to solve human problems formalized into [[a subset of computer science]].\n\nAI has changed the original computational statistics paradigm to the modern idea that machines could mimic actual human capabilities, such as decision making and performing more \"human\" tasks.\n\nModern AI into two categories:\n1. **General AI**: Planning, decision making, identifying objects, recognizing sounds, social \u0026 business interactions\n2. **Applied AI**: Driverless / Autonomous car or machine smartly trade stocks\n\n## Machine Learning\n\nInstead of engineers \"teaching\" or programming computers to have what they need to carry out tasks, that perhaps computers could teach themselves - **learn something without being explicitly programmed to do so**.\n\n**ML (Machine Learning) Is a form of AI where based on more data**, and they can change actions and response, which will make more efficient, adaptable and scalable. E.g., navigation apps and recomendation engines.\n\nML Classifiec into:\n1. Supervised\n2. Unsupervised\n3. Reinforcement Learning\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n## Data Science\n\nData science has many tools, techniques and algorithms called from these fields to handle big data. The goal of data science, somewhat similar to machine learning, is to make accurate predictions and to automate and perform transactions in real-time, such as purchasing internet traffic or automatically generating content.\n\nData science relies less on math and coding and more on data and building new systems to process the data. Relying on the fields of data integration, distributed architechture, automated machine learning, data visualization, data engineering and automated data-driven decisions. Data science can cover an entire spectrum of data pprocessing, not only the algorithms or statistics related to data.\n\n## Deep Learning\n\nDeep learning is a technique for implementing ML. ML provides the desired output from a given input, but DL (Deep Learning) reads the input and applies it to another data. \n\nIn ML, we can easily classify the flower based upon the features. Suppose you want a machine to look at an image and determine what it represents to human eye, wherer a face, flower, landscape, truck, building, etc. ML is not sufficient for this task because machine learning can only produce an output from a data set - whether according to a known algorithm or based on the inherent structure of the data. You might be able to use machine learning to determine to use ML to determine whether an image was of an \"X\" - a flower, say - and it would learn and get more accurate. But that output is binary (yes/no) and is dependent on the algorithm, not the data.\n\nIn the image recognition case, the outcome is not binary and not dependent on the algorithm. The neural network perform micro calculations with computational on many layers. Neural networks also support weighting data for confidence. These results in  a probabilistic system, vs. deterministics, and can handle tasks that we think of as requiring more \"human-like\" judgement.Z\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/difference-between-logistic-and-linear-regression":{"title":"difference-between-logistic-and-linear-regression","content":"\nLinear and Logistic regression are the most basic form of regression which are commonly used. \n\n| Linear                                                                                  | Logistic                                            |\n|-----------------------------------------------------------------------------------------|-----------------------------------------------------|\n| dependent variable is continuous                                                        | dependent variable is binary                        |\n| requires to establish the linear relationship among dependent and independent variables | not necessary for logistic regression               |\n| independent variable can be correlated with each other                                  | the variable must not be correlated with each other |\n\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/difference-between-supervised-unsupervised-reinforcement-learning":{"title":"What is the difference between supervised, unsupervised, and reinforcement learning?","content":"\n## Supervised Learning\n\nIn a supervised learning model, the algorithm learns on **a labeled dataset**, to generate reasonable predictions for the response to new data. Ex:\n- Regression\n- Classification\n\n## Unsupervised Learning\n\nAn unsupervised learning model in contrast provides unlabelled data that the algorithm tries to make sense of by extracting features, co-occurrence and underlying patterns on its own. Ex:\n- Clustering\n- Anomaly detection\n- Associations\n- Autoencoders\n\n## Reinforcement Learning\n\nReinforcement learning is less supervised and depends on the learning agent in determining the output solutions by arriving at different possible ways to achieve the best possible solution.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/ds-hack":{"title":"ds-hack","content":"\n- save tabular files to parquet format [youtube](https://www.youtube.com/watch?v=u4rsA5ZiTls)\n- change string column to category\n- downcasting integers\n\t- int8 range : -128 to 127\n\t- int16 range: -32678 to 32676\n- downcasting float to float32 (check some values behind decimals)\n- change yes no column to boolean\n- speed up pandas df looping -\u003e apply() -\u003e vectorized","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/ds-interview":{"title":"ds-interview","content":"\nTechnical coding test\n- SQL -\u003e [[code-sql]]\n- Python\n\nPast project detail\n- speaker diarization\n- madani\n- knowledge\n\t- clustering\n\t- cnn\n\t- time series\n\t- survival analysis\n\t- data viz\n\nGeneral AI ML question\n\nGeneral stats question\n- statistical significance\n- [[hypothesis-testing]] testing\n\t- 2 types of error, the consequence, which one is better for what case\n- pvalue\n- distribution\n- t-test\n\nGeneral ML Algorithm\n- regression\n\t- linear\n\t- lasso\n\t- ridge\n- tree based\n\t- decision tree\n\t- random forest\n- unsupervised learning\n\t- clustering\n\t- association\n\nPersonal goals\n- me in the next 5 years\n- kelebihan kekurangan\n\nDeep learning question \n- CNN\n- loss function\n- optimizer\n- forward backward prop","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/ds-ml-step":{"title":"ds-ml-step","content":"\nML steps:\n- determine which type of ML problems we would like to solve\n- gather data\n- [[feature-engineering]]\n- \n\n# Reference\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/ds-plotting":{"title":"ds-plotting","content":"\n## Read csv\n\n- parse_dates\n- index_col\n\n## Plotting in notebook setup\n\n```python\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n```\n\n## Define Your Viz Goals\n\n- Trends: lineplot\n- Relationship: scatterplot, lmplot,  regplot, heatmap, swarmplot \n- Distribution: histogram, kde, 2d kde\n- Compare values: barplot\n\n## Plot Option Based on Data Types\n\n- date time + multiple variable: line plot\n- 1 numeric: histogram, kde\n- 1 numeric multiple categorical: histogram\n- 2 numeric: scatterplot, 2d kde\n- 2 numeric + 1 categorical: scatterplot, lm regression plot\n- 1 categorical + 1 numeric: barplot, swarmplot\n- 2 categorical + 1 numeric: histogram, kde, barplot, heatmaps\n\n\n## Line Plot\n\n```python\nplt.figure(figsize=(14,6))\n\nplt.title(\"Daily Global Streams of Popular Songs in 2017-2018\")\n\nsns.lineplot(data=spotify_data['Shape of You'], label=\"Shape of You\")\nsns.lineplot(data=spotify_data['Despacito'], label=\"Despacito\")\n\nplt.xlabel(\"Date\")\n```\n\n## Bar Plot\n\n```python\nplt.figure(figsize=(14,7))\n\nsns.barplot(x=flight_data.index, y=flight_data['NK'])\nplt.ylabel(\"Arrival delay (in minutes)\")\n```\n\n## Heatmaps\n\n```python\nsns.heatmap(data=flight_data, annot=True)\n```\n\n## Scatter Plot\n\n```python\n# color by smoker categorical data\nsns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'], hue=insurance_data['smoker'])\n\n# add regression line\nsns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])\n# colored by smoker categorical data, create 2 regression line\nsns.lmplot(x=\"bmi\", y=\"charges\", hue=\"smoker\", data=insurance_data)\n```\n\n### Categorical Scatter Plot\n\n```python\n\n# smoker=category, charges=numeric\nsns.swarmplot(x=insurance_data['smoker'],\n              y=insurance_data['charges'])\n```\n\n## Histogram\n\n```python\nsns.histplot(data=iris_data, x='Petal Length (cm)', hue='Species')\n```\n\n### KDE (Kernel Density Estimation) Plot\n\n\u003e smoothed histogram\n\n```python\nsns.kdeplot(data=iris_data, x='Petal Length (cm)', hue='Species', shade=True)\n```\n\n### 2D KDE (Kernel Density Estimation) Plot\n\n```python\nsns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind=\"kde\")\n```","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/ds-skill":{"title":"ds-skill","content":"\n- read datas\n\t- check dtypes (handling datetime data, index_col)\n\t- check memory usage\n- summary data\n\t- summary distribution\n\t- summary between some categorical variable\n- [[feature-engineering]]\n\t- [[detect-extreme-values]]\n\t- [[handling-missing-values]]\n- [[hyper-parameter-tuning]]","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/entropy":{"title":"entropy","content":"\n\u003e In statistics, entropy is a measure of information\n\n![](entropy-1.png)\n\n- varies from 0 to 1\n\t- 0: all the data belong to a single class\n\t- 1: the class distribution is equal\n- a measure of the amount of uncertainity in the data set\n- when $H(S)=0$, the set is perfectly classified (all element in $S$ are of the same class)\n\n---\n\nLet’s assume that a dataset T associated with a node contains examples from n classes. Then, its entropy is:\n\n![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-004399bf736b5463087bdd15d256e30e_l3.svg)\n\nwhere $p_j$ is the relative frequency of class $j$ in $T$. \n\nAs is the case with the [[gini-impurity-index]], a node is pure when $entropy(T)$ takes its minimum value, zero, and impure when it takes its highest value, 1.\n\n## Example\n\n- 4 red, 0 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ba59773932c9932bd2af5ee6be657e67_l3.svg)\n- 2 red, 2 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-f8f46c401aa3e27c2e16351823618aa8_l3.svg)\n- 3 red, 1 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-6a910318c70392856b99697b48b0ecec_l3.svg)\n\n## Information Gain\n\n\u003e The information gain is the difference between a **parent node’s entropy** and **the weighted sum of its child node entropies**.\n\nLet’s assume a dataset $T$ with $N$ objects is partitioned into two datasets: $T_1$ and $T_2$ of sizes $N_1$ and $N_2$. **Then, the split’s Information Gain ($Gain_{split}$) is**:\n\n![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-652f1dd07771006ca61719507ae7784a_l3.svg)\n\nIn general, if splitting $T$ into m subsets $T_1, T_2, \\ldots, T_m$ with $N_1, N_2, \\ldots, N_m$ objects, respectively, the split’s Information Gain ($Gain_{split}$) is:\n\n![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-1c53327d676f878366ba10c245666a00_l3.svg)\n\n### Example Splitting by Information Gain\n\nhttps://www.baeldung.com/cs/impurity-entropy-gini-index#2-example-splitting-by-information-gain\n\nsteps:\n- determine the attribute that offers the highest Information Gain\n\n\n## References\n- https://www.baeldung.com/cs/impurity-entropy-gini-index\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/feature-engineering":{"title":"feature-engineering","content":"\n\u003e a group of activities that trasnform data into desired format\n\nexample:\n- splitting data into training and testing\n- handle missing values\n- encode categorical attributes\n\nthe outermost interface that we interact with the model\n# Reference\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/fitting":{"title":"fitting","content":"\n\u003e The step of capturing patterns from data is called **fitting**","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/general-architechture-of-machine-learning":{"title":"General Architechture of Machine Learning","content":"\n![](/images/general-architechture-of-machine-learning-1.png)\n\n**Business Understanding**: Understand the give use case and also it's good to know more about the domain for which the use cases are built.\n\n**Data Acquisition and Understanding**: Data gathering from different sources and understanding the data. Cleaning the data, handling the missing data if any, data wrangling, and EDA (Exploratory Data Analysis)\n\n\u003e Read More: [My Sample Material EDA Course on Sepuluh Nopember Institut of Technology](https://aradinka.vercel.app/blog/post-asdos)\n\n**Modeling**: Feature engineering, scaling the data, feature selection. Backward elimination method, correlation factors, PCA, and domain knowledge to select the features. Model training (Based on trial and error method of by experience, we select the algorithm and train with selected features). Model evaluation (Accuracy of the model, confusion matrix and cross-validation).\n\n**Deployment**: Once the model has good performance, we deploy the model in the cloud. Once we deploy, we monitor the performance of the model. If it's good, we go live with the model or reiterate all the process until our model performance is good.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/gini-impurity-index":{"title":"gini-impurity-index","content":"\n\u003e Gini Index is related to the **misclassification probability** of a random sample\n\nGini Index may result in values inside the interval [0, 0.5]\n- The minimum value of zero corresponds to **a node containing the elements of the same class**. In case this occurs, the node is called **pure**\n- The maximum value of 0.5 corresponds to the highest impurity of a node. Called **impure**\n\n# Example\n\nIn this example, we’ll compute the Gini Indices for 3 different cases of a set with 4 balls of two different colors, red and blue\n\n- 4 red, 0 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-03178ed050fa7f5ac3d42af8f8535842_l3.svg)\n- 2 red, 2 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-feb5ec6ce739f898c6c6ff05c2a9b904_l3.svg)\n- 3 red, 1 blue\n\t- ![](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-26774e1805b1b0048d64045c59ca7593_l3.svg)\n\n# Reference\n- https://www.baeldung.com/cs/impurity-entropy-gini-index","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/great-videos":{"title":"great-videos","content":"\n[inside-ali-abdaal-second-brain](inside-ali-abdaal-second-brain.md)\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/hyper-parameter-tuning":{"title":"hyper-parameter-tuning","content":"\n\u003e The reason that it is highlighted as 'hyper' is because the parameters that we tune are **the outermost interface that we interact with the model**, which would eventually have impacts on the underlying parameters of the model\n\n\n","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/hypothesis-testing":{"title":"hypothesis","content":"\n## What is hypothesis?\n\n\u003e A hypothesis is a concept that is not yet verified, but if true would explain certain facts or phenomena\n\n## What is hypothesis testing?\n\nIt is an **educated guess** about something in the world around you. It should be **testable**, either by experiment or observation.\n\nIn hypothesis testing, we have to make two hypotheses i.e Null hypothesis and the alternative hypothesis\n\n### Null Hypothesis\n\nThe null hypothesis is the one that states that sample observations result purely from change.\n\n### Alternative Hypothesis\n\nThe alternative hypothesis challenges the null hypothesis and is basically **a hypothesis that the researcher believes to be true**.\n\n## Two types of error in hypothesis testing\n\n### Type I error\n\n\u003e We reject the null hypothesis when it is true\n\nThat is we accept the variant B when it is not performing better than A\n\n### Type II error\n\n\u003e We failed to reject the null hypothesis when it is false","lastmodified":"2022-11-26T16:30:16.861961329Z","tags":null},"/impurity":{"title":"impurity","content":"\n\u003e impurity is a measure of **homogeneity** of the labels at the node at hand\n\n\nDefine impurity:\n- There are different ways to define impurity. In classification tasks, we frequently use the [[gini-impurity-index]] and [[entropy]]","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/inside-ali-abdaal-second-brain":{"title":"inside-ali-abdaal-second-brain","content":"\nhttps://youtu.be/-Y_6U0FoqDk\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Y_6U0FoqDk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/l1-regularization":{"title":"What is l1 regularization?","content":"\nThe main objective of creating a model (training data) is making sure it fits the data properly and reduce the loss. Sometimes the model that is trained may fail and give a poor performance during analyzing the test data. This leads to overfittng. Regularization came to overcome overfitting.\n\nLasso regression (Least Absolute Shrinkage and Selection Operator) adds \"Absoule value of magnitude\" of coefficient, as penalty term to the loss function. Lasso shrinks the less important features coefficient to zero, thus removing some feature altogether. So this works well for feature selection in case we have a huge number of features.\n\n\nMethods like cross-validation, stepwise regression are there to handle overfitting and perform feature selection work well with a small set of features. There techniques are good when we are dealing with a large set of features.\n\nAlong with shrinking coefficients, the lasso performs feature selection as well. Because some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/l2-regularization":{"title":"What is l2 regularization?","content":"\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn't perform well on new/unseen data on which model wasn't trained on. To avoid overfitting, we do cross-validation sampling, reducing the number of features, pruning, regularization, etc.\n\nThe regressioin model that uses L2 regularization is called ridge regression. Regularization adds the penalty as model complexity increases. The regularization parameter ($\\lambda$) penalizes all the parameters except intercept so that the model generalized the data and won't overfit.\n\nRidge regression add \"squared magnitude of the coefficient\" as penalty term to the loss function. If the $\\lambda$ is zero, then it is equivalent to [OLS](ordinary-least-square-model.md). But if the lambda is very large, then it will add too much weight, and it will lead to underfitting.\n\nRidge regularization forces the weights to be small but doesn't not make them zero, and doesn't give the sparse solution. Ridge is not robust to outliers as square terms blow up the error differences of the outliers, and the regularization term tries to fix it by penalizing the weights.\n\nRidge regression performs better when all the input features influence the output, and all with weights are of roughly equal size. L2 regularization can learn complex data patterns.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/linear-regression":{"title":"What is linear regression?","content":"\n## Linear Regressions\n\nLinear regressions tends to establish a relationship between a dependent variable `Y` and one or more independent variable `X` by finding the best fit of the straight line.\n\nThe equation for linear model is $Y = mX + c$, where $m$ is the slope and $c$ is the intercept.\n\n![](images/linear-regression-1.png)\n\nIn the above diagram, the blue dots we see are the distribution of `y` with respect to `x`. There is no straight line that runs through all the data points. So, the objective here is to fit the best fit of a straight line that will try to minimize the error between the expected and actual value.\n\nRead more:\n- [What is ordinary least square model?](ordinary-least-square-model.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/logistic-regression":{"title":"logistic-regression","content":"\nThe logistic regression technique involves the dependent variable, which can be represented in the binary (0 or 1, true or false, yes or no) values, which means that **the outcome could only be in either one form of two**.\n\n","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/loss-function":{"title":"loss-function","content":"\n\u003e the cost that is incurred by the difference between the prediction and the true value. The larger the difference, the bigger the loss.","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/machine-learning":{"title":"What is machine learning?","content":"\nMachine learning is the scientific study of algorithms and statistical models that computer systems use to affectively perform a specific task **without using explicit instruction**, relyring on patterns and inference instead.\n\nBuilding a model by learning the patterns of historical data with some relationship between data to make a data-driven prediction.\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n# Training Data\n\n\u003e The relationship between the data and the machine learning model, is as critical as the fuel to the engine of rocket.\n\n\u003e It is not exaggerating to say that the data dictates how the machine learning model is built.\n\n## garbage in, garbage out\n\nGroup of blind men, who have never come across an elephant before, would like to learn and conceptualize what an elephant is like by touching it. \n\nEach man touches a part of the body, such as leg, tusk or tail _etc_. While each of them got a part of the reality, none of them has the whole picture of an elephant. Therefore, **none of them actually learned the true image of an elephant**.\n\n![](https://assets.leetcode.com/uploads/2018/12/16/card_elephant.png)\n\nThe training data we got could be those images of legs or tusks from an elephant, while during the test processing, the testing data we got are the full portraits of elephants. \n\nIt would not be surprising to find out that our trained model does not perform well in this case, since we do not have the _**high-quality**_ training data that is closer to the reality in the first place.\n\nIf the data is really important, then why not feeding the \"high-quality\" data such as full portraits of elephants into the algorithm, instead of snapshots on parts of the elephant body?\n- Because, facing a problem, we or the machine, like the \"blind-men\", **often struggle to gather the data** that captures the essential characteristics of the problem, either due to the technical issues (_e.g._ data privacy) or simply because we do not perceive the problem in the right way.\n\n## Real world data\n\nIn the real world, the data we got reflects a part of reality in a favorable case, or it could be some noise in a less favorable case, or in the worst case, even a contradiction to the reality. \n\nRegardless of the machine learning algorithms, one would not be able to learn anything from data that contains too much noise or is too inconsistent with the reality.\n\n# Workflow\n\n\u003e The relationship between the data and the machine learning model, is as critical as the fuel to the engine of rocket.\n\n## Data-Centric Workflow\n\n![](https://assets.leetcode.com/uploads/2018/11/25/ml_workflow.pngthe outermost interface that we interact with the model)\n\n# Reference\n\n- https://assets.leetcode.com/uploads/2018/12/16/card_elephant.png","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/machine-learning-algorithm":{"title":"machine-learning-algorithm","content":"\nA machine learning algorithm is the process that uncovers the underlying relationship whith the data\n\nThe outcome of a machine learning algorithm is called [[machine-learning-model]], which can be considered as a `function` $F$, which outputs certain results, when given the input.\n\nReference:\n- https://leetcode.com/explore/featured/card/machine-learning-101/287/what_is_ml/1617/","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/machine-learning-model":{"title":"machine-learning-model","content":"\nRather than a predifined and fixed function, a machine learning model is dedrived from historical data. Therefore, when fed with with different data, the output of [[machine-learning-algorithm]] changes, i.e the machine learning model changes.\n\nReference:\n- https://leetcode.com/explore/featured/card/machine-learning-101/287/what_is_ml/1617/","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/main-prediction":{"title":"main-prediction","content":"\n\u003e a learner is playing a dart-throwing game\n\n\u003e _how many points that the player would score?_\n\nhttps://leetcode.com/explore/featured/card/machine-learning-101/281/how_to_ml/2642/\n\n","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/mean-absolute-error":{"title":"mean-absolute-error","content":"\n## Interpretation\n\n\u003e On average, our predictions are off by about X","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/mean-square-error":{"title":"What is mean square error?","content":"\nThe mean squared error tells you how close a regression line is to a set of points. It does this by taking the distance from the points to the regression line (these distance are the \"errors\") and squaring them.\n\nGiving an intuition\n\n![](images/mean-square-error-1.png)\n\nThe line equation in `Y = mX + c`. We want to find `m` (slope) and `c` (intercept) that minimize the squared error.\n\n$$\nMSE = \\frac{1}{n} \\displaystyle\\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2\n$$\n\nRead more: [Linear Regression](linear-regression.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/ordinary-least-square-model":{"title":"What is ordinary least square model?","content":"\nOLS (Ordinary Least Square) is a stats model, which will help us in identifying the more significant features that can has an influence on the output.\n\nOLS model in python is executed as:\n\n```python\nlm = smf.ols(formula='Sales ~ am + constant', data=data).fit()\nlm.conf_int()\nlm.summary()\n```\n\nand we got the output as below,\n\n![](images/ordinary-least-square-model-1.png)\n\nThe higher the `t-value` for the feature, the more significant the feature is to the output variable. The `p-value` plays a rule in rejecting the null hypothesis. If the `p-value` is less than `0.05` (95% confidence interval) for a feature, then we can consider the feature to be significant.","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/overfitting":{"title":"overfitting","content":"\n\u003e An overfitting model is the one that fits well with the training data, _i.e._ little or no error, however it does not generalized well to the unseen data\n\n\u003e a model matches the training data almost perfectly, but does poorly in validation and other new data\n\n\u003e overfitting capturing spurious (false) patterns that won't recur in the future, leading to less accurate predictions\n\nHow to avoid overfitting?\n- try out another algorithm that could **generate a simpler model** from the training data set\n- adds a [[regularization]] term to the algorithm, _i.e._ penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data\n","lastmodified":"2022-11-26T16:30:16.909964204Z","tags":null},"/r-squared":{"title":"What is R squared?","content":"\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. \n\nR-squared explained `variation / total variation`. R-squared is always between 0% and 100%. 0% indicates that the model explains none of the variability of the response data around it's mean. 100% indicates that the model explains all the variability of the response data around it's mean. In general, the higher the R-squared, the better the model fits your data.\n\n$$\nR^2 = 1 - \\frac{SS_{Regression}}{SS_{Total}} = \\frac{\\sum_i{(y_i - \\hat{y_i})^2}}{\\sum_i{(y_i - \\bar{y_i})^2}}\n$$\n\nThere is a problem with the R-squared. The problem araises when we ask this question to ourselves. \"Is it good to help as many independent variable as possible?\" The answer is No, because we understood that each independent variable should have a meaningful impact. \n\nBut, even if we add independent variables which are not meaningfully. will it improve R-squared value? Yes, this is the basic problem with R-squared. How many junk independent variables or important independent variable or impactful independent variable you add to your model, **the R-squared value will always increase**. It will never decrease with the addition of a newly independent variable, whether it could be an impactful, non-impactful, or bad variable. So we need another way to measure equivalent R-squared, which panilizes our model with any junk independent variable.\n\nSo, we calculate the Adjusted R-squared with a better adjustment in the formula of generic R-squared.\n\n$$\nR^2 adjusted = 1 - \\frac{(1-R^2)(N-1)}{N-p-1}\n$$\n\nwhere, $R^2$ is the sample R-square, $p$ is the number of predictors, and $N$ is the total sample size.","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/random-forest":{"title":"random-forest","content":"\n# Advantages \u0026 Disadvantages\n\n## Advantages\n\n- Reduces [[overfitting]]\n- Higher accuracy compared to other models\n\n\u003e Random Forests address both [[decision-tree]] issues ([[overfitting]] and instability). The idea is to **construct multiple trees using different subsets of the training data and features** for each tree in the forest. Then, **aggregate** their predictions by outputting the majority vote or the average value\n\nThe rationale is that **an ensemble of models is likely more accurate than a single tree**. So, even if a tree overfits its subset, we expect other trees in the forest to compensate for it. That’s why we use different subsets for each tree, to force them to approach the problem from different angles.\n\nThe random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree.\n\n## Disadvantages\n\n- Training complexity can be high\n- Not very interpretable\n\n\u003e **A single tree is interpretable, whereas a forest is not**\n\nHumans can visualize and understand a tree, no matter if they’re machine learning experts or laypeople. That’s not the case with forests. They contain a lot of trees, so **explaining how they output the aggregated predictions is very hard,** if not impossible.\n\n# Reference\n- https://www.baeldung.com/cs/decision-trees-vs-random-forests\n- https://www.kaggle.com/code/dansbecker/random-forests/tutorial\n","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/random-sampling":{"title":"random-sampling","content":"\n\u003e It's a technique where each sample in a population has an equal change of being chosen\n\nIt eliminates sampling bias -\u003e We want our sample to be representative of the entire population rather the sample itself","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/random-state":{"title":"random-state","content":"\nMany machine learning models allow some randomness in model training. Specifying a number for `random_state` ensures you get the same results in each run. This is considered a good practice. You use any number, and model quality won't depend meaningfully on exactly what value you choose.","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/regularization":{"title":"regularization","content":"\n\u003e penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data\n\n","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/semi-supervised-learning":{"title":"semi-supervised-learning","content":"\nThe data set is massive but the labeled sample are few\n\nTraining strategy:\n- we train a model with the labeled data, then we apply the model to predict the unlabeled data\n- first cluster the images into groups (unsupervised learning), and then apply the supervised learning algorithm on each of the groups individually\n\n\n\n# Reference","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/standard-deviation":{"title":"standard-deviation","content":"\n\u003e standard deviation measures how numerically spread out the values are","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/supervised-learning":{"title":"supervised-learning","content":"\n\u003e An important measurement for supervised learning algorithms, is the _**generalization**_, which measures how well that a model derived from the training data can predict the desired attribute of the unseen data.\n\n","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/support-vector-regression":{"title":"What is support vector regression?","content":"\nWhy SVR (support vector regression)? What is the difference between SVR and a simple regression model?\n\nThe answer is, simple linear regression try to minimize the error rate. But in SVR, we try to fit the error within a certain threshold.\n\n![](images/support-vector-regression-1.png)\n\nMain concept:\n- Boundary Line (Red Line)\n- Kernel\n- Support Vector\n- Hyperplane (Blue Line)\n\nOur best fit line is the one where the hyperplane has the maximum number of points. What we are trying to do here is to decide a decision boundary at \"e\" distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are whithin that boundary line.\n\n\n![](images/support-vector-regression-2.png)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/tags-list":{"title":"tags-list","content":"\n- all-post\n- algorithm\n- what-is","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/tags-system":{"title":"Tags System","content":"\n# PARA Methods\n\nP.A.R.A. stands for **Projects — Areas — Resources — Archives**, the four top-level categories that encompass every type of information you might encounter in your work and life.\n\n- Projects: a series of tasks linked to a goal\n- Area: a sphere of activity with a standard to be maintained over time\n- Resource: a topic or theme of ongoing interest\n- Archive: inactive items from the other three categories\n\nexample tags:\n- project-project_name\n- area-ds\n\nResource:\n- [fortelabs](https://fortelabs.com/blog/para/)","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/tags/authors-ali-abdaal":{"title":"","content":"","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/under-coverage-bias":{"title":"under-coverage-bias","content":"\n\u003e It is the bias from sampling too few observations","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/underfitting":{"title":"underfitting","content":"\n\u003e An underfitting model is the one that does not fit well with the training data, _i.e._ significantly deviated from the ground truth\n\n\u003e Underfitting **failing to capture relevant patterns**, again leading to less accurate predictions\n\nCauses:\n- the model is over-simplified for the data, therefore it is not capable to capture the hidden relationship within the data\n\n![](https://assets.leetcode.com/uploads/2019/01/01/underfitting.png)\n\n\u003e Read also: [[underfitting-overfitting]]","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/underfitting-overfitting":{"title":"underfitting-overfitting","content":"\n\u003e When we say a model is underfitting or overfitting, it implies that the model does not generalized well to the unseen data.\n\nwhy a model that fits well with the training data does not necessarily imply that it would generalize well to the unseen data? because...\n- **the training data are just samples we collect from the real world**, which represents only a proportion of reality. It could be the case that the training data is simply not representative, thus even the model fits perfectly the training data, it would not fit well with the unseen data\n- **the data that we collect contains noises and errors inevitably**. The model that fits perfectly with the data, would also capture the undesired noises and errors by mistake, which would eventually lead to bias and errors in the prediction for the unseen data\n\n![](https://assets.leetcode.com/uploads/2019/01/01/underfitting.png)\n\n\u003e Read also: [[underfitting]]","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/variance":{"title":"variance","content":"\n\u003e variance is the tendency to learn random things unrelated to the real signal","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null},"/vim-bindings":{"title":"vim-bindings","content":"\n- move around: h, j, k, l\n- go to the start / end of a line: 0, $\n- move between words: w, e, b\n- find the next/before the same word: *, #\n- move to to the beginning of the line and back to end: gg, G\n- go to line 2: 2G\n- search text: / text\n\t- n next search\n\t- N previous search\n- create new line after / before: o, O\n- detele character under cursor and to the left of the cursor: x, X\n- delete line: dd\n- select multiple letter to delete (using visual mode): v, e, d\n- undo: u\n- redo: crtl + r\n- help: :help\n- save: :w\n- quit: :q\n- quit without saving: :q!\n\n\ntsett tset \n","lastmodified":"2022-11-26T16:30:16.913964444Z","tags":null}}