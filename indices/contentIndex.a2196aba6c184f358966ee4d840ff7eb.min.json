{"/":{"title":"Aradinka | Digital Garden","content":"\nThis website meant to be my digital garden built using [Quartz](https://github.com/jackyzha0/quartz).\n\nCheck out my portfolio **[aradinka.com](https://aradinka.com)**\n\n[All Post](/tags/all-post)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/_areas":{"title":"_areas","content":"\n[Productivity](/tags/areas-productivity)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/_authors":{"title":"_authors","content":"\n[Ali Abdaal](/tags/authors-ali-abdaal)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/batch-size":{"title":"batch-size","content":"\nBaca tentang [[batch-nomalization]]\n\nBatch size salah satu yang bisa kita tuning. Tuning batch sizenya sampai kita mendapatkan performa yang maksimum \n\nPengaruh batch size pada performa model:\n- Terlalu kecil: Distribusi hasil batch normalization kurang bagus. Kadang skew ke kiri kadang ke kanan\n- Terlalu besar: Hessian matrix tidak mencerminkan local minima\n\n- Apabila menggunakan batch size besar, gunakan [[learning-rate]] yang kecil\n\n\n[Innovation Day: MultiGPU Infrastructure \u0026 Implementation](https://youtu.be/IayDLHyHqlE?t=3574)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/bias-vs-variance":{"title":"bias-vs-variance","content":"\nIt provides another perspective to look at the phenomenon of [[underfitting]] and [[overfitting]]","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/convolutional-neural-network":{"title":"convolutional-neural-network","content":"\nKenapa cnn bagus untuk dipakai di data image, text, audio dan video?\n- Mencerminkan spatial correlation between features\n- Di data image: Pixel di titik `i, j` sangat berkaitan dengan `i+1, j+1`. Dan sekitarnya1\n- Di data text: Text sebelum dan sesudah saling berhubungan\n\n[](https://youtu.be/IayDLHyHqlE?t=3725)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/difference-between-ai-ds-ml-dl":{"title":"What is the difference between AI, DS, ML and DL?","content":"\n## Artificial Intelligence\n\n**AI is purely math and scientifc exercise**, but when it became computational, it started to solve human problems formalized into [[a subset of computer science]].\n\nAI has changed the original computational statistics paradigm to the modern idea that machines could mimic actual human capabilities, such as decision making and performing more \"human\" tasks.\n\nModern AI into two categories:\n1. **General AI**: Planning, decision making, identifying objects, recognizing sounds, social \u0026 business interactions\n2. **Applied AI**: Driverless / Autonomous car or machine smartly trade stocks\n\n## Machine Learning\n\nInstead of engineers \"teaching\" or programming computers to have what they need to carry out tasks, that perhaps computers could teach themselves - **learn something without being explicitly programmed to do so**.\n\n**ML (Machine Learning) Is a form of AI where based on more data**, and they can change actions and response, which will make more efficient, adaptable and scalable. E.g., navigation apps and recomendation engines.\n\nML Classifiec into:\n1. Supervised\n2. Unsupervised\n3. Reinforcement Learning\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n## Data Science\n\nData science has many tools, techniques and algorithms called from these fields to handle big data. The goal of data science, somewhat similar to machine learning, is to make accurate predictions and to automate and perform transactions in real-time, such as purchasing internet traffic or automatically generating content.\n\nData science relies less on math and coding and more on data and building new systems to process the data. Relying on the fields of data integration, distributed architechture, automated machine learning, data visualization, data engineering and automated data-driven decisions. Data science can cover an entire spectrum of data pprocessing, not only the algorithms or statistics related to data.\n\n## Deep Learning\n\nDeep learning is a technique for implementing ML. ML provides the desired output from a given input, but DL (Deep Learning) reads the input and applies it to another data. \n\nIn ML, we can easily classify the flower based upon the features. Suppose you want a machine to look at an image and determine what it represents to human eye, wherer a face, flower, landscape, truck, building, etc. ML is not sufficient for this task because machine learning can only produce an output from a data set - whether according to a known algorithm or based on the inherent structure of the data. You might be able to use machine learning to determine to use ML to determine whether an image was of an \"X\" - a flower, say - and it would learn and get more accurate. But that output is binary (yes/no) and is dependent on the algorithm, not the data.\n\nIn the image recognition case, the outcome is not binary and not dependent on the algorithm. The neural network perform micro calculations with computational on many layers. Neural networks also support weighting data for confidence. These results in  a probabilistic system, vs. deterministics, and can handle tasks that we think of as requiring more \"human-like\" judgement.Z\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/difference-between-supervised-unsupervised-reinforcement-learning":{"title":"What is the difference between supervised, unsupervised, and reinforcement learning?","content":"\n## Supervised Learning\n\nIn a supervised learning model, the algorithm learns on **a labeled dataset**, to generate reasonable predictions for the response to new data. Ex:\n- Regression\n- Classification\n\n## Unsupervised Learning\n\nAn unsupervised learning model in contrast provides unlabelled data that the algorithm tries to make sense of by extracting features, co-occurrence and underlying patterns on its own. Ex:\n- Clustering\n- Anomaly detection\n- Associations\n- Autoencoders\n\n## Reinforcement Learning\n\nReinforcement learning is less supervised and depends on the learning agent in determining the output solutions by arriving at different possible ways to achieve the best possible solution.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)\n","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/ds-ml-step":{"title":"ds-ml-step","content":"\nML steps:\n- determine which type of ML problems we would like to solve\n- gather data\n- [[feature-engineering]]\n- \n\n# Reference\n","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/ds-skill":{"title":"ds-skill","content":"\n- [[feature-engineering]]\n- [[hyper-parameter-tuning]]","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/feature-engineering":{"title":"feature-engineering","content":"\n\u003e a group of activities that trasnform data into desired format\n\nexample:\n- splitting data into training and testing\n- handle missing values\n- encode categorical attributes\n\nthe outermost interface that we interact with the model\n# Reference\n","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/general-architechture-of-machine-learning":{"title":"General Architechture of Machine Learning","content":"\n![](/images/general-architechture-of-machine-learning-1.png)\n\n**Business Understanding**: Understand the give use case and also it's good to know more about the domain for which the use cases are built.\n\n**Data Acquisition and Understanding**: Data gathering from different sources and understanding the data. Cleaning the data, handling the missing data if any, data wrangling, and EDA (Exploratory Data Analysis)\n\n\u003e Read More: [My Sample Material EDA Course on Sepuluh Nopember Institut of Technology](https://aradinka.vercel.app/blog/post-asdos)\n\n**Modeling**: Feature engineering, scaling the data, feature selection. Backward elimination method, correlation factors, PCA, and domain knowledge to select the features. Model training (Based on trial and error method of by experience, we select the algorithm and train with selected features). Model evaluation (Accuracy of the model, confusion matrix and cross-validation).\n\n**Deployment**: Once the model has good performance, we deploy the model in the cloud. Once we deploy, we monitor the performance of the model. If it's good, we go live with the model or reiterate all the process until our model performance is good.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/great-videos":{"title":"great-videos","content":"\n[inside-ali-abdaal-second-brain](inside-ali-abdaal-second-brain.md)\n","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/hyper-parameter-tuning":{"title":"hyper-parameter-tuning","content":"\n\u003e The reason that it is highlighted as 'hyper' is because the parameters that we tune are **the outermost interface that we interact with the model**, which would eventually have impacts on the underlying parameters of the model\n\n\n","lastmodified":"2022-11-21T09:39:28.094329189Z","tags":null},"/inside-ali-abdaal-second-brain":{"title":"inside-ali-abdaal-second-brain","content":"\nhttps://youtu.be/-Y_6U0FoqDk\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Y_6U0FoqDk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/l1-regularization":{"title":"What is l1 regularization?","content":"\nThe main objective of creating a model (training data) is making sure it fits the data properly and reduce the loss. Sometimes the model that is trained may fail and give a poor performance during analyzing the test data. This leads to overfittng. Regularization came to overcome overfitting.\n\nLasso regression (Least Absolute Shrinkage and Selection Operator) adds \"Absoule value of magnitude\" of coefficient, as penalty term to the loss function. Lasso shrinks the less important features coefficient to zero, thus removing some feature altogether. So this works well for feature selection in case we have a huge number of features.\n\n\nMethods like cross-validation, stepwise regression are there to handle overfitting and perform feature selection work well with a small set of features. There techniques are good when we are dealing with a large set of features.\n\nAlong with shrinking coefficients, the lasso performs feature selection as well. Because some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/l2-regularization":{"title":"What is l2 regularization?","content":"\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn't perform well on new/unseen data on which model wasn't trained on. To avoid overfitting, we do cross-validation sampling, reducing the number of features, pruning, regularization, etc.\n\nThe regressioin model that uses L2 regularization is called ridge regression. Regularization adds the penalty as model complexity increases. The regularization parameter ($\\lambda$) penalizes all the parameters except intercept so that the model generalized the data and won't overfit.\n\nRidge regression add \"squared magnitude of the coefficient\" as penalty term to the loss function. If the $\\lambda$ is zero, then it is equivalent to [OLS](ordinary-least-square-model.md). But if the lambda is very large, then it will add too much weight, and it will lead to underfitting.\n\nRidge regularization forces the weights to be small but doesn't not make them zero, and doesn't give the sparse solution. Ridge is not robust to outliers as square terms blow up the error differences of the outliers, and the regularization term tries to fix it by penalizing the weights.\n\nRidge regression performs better when all the input features influence the output, and all with weights are of roughly equal size. L2 regularization can learn complex data patterns.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/linear-regression":{"title":"What is linear regression?","content":"\n## Linear Regressions\n\nLinear regressions tends to establish a relationship between a dependent variable `Y` and one or more independent variable `X` by finding the best fit of the straight line.\n\nThe equation for linear model is $Y = mX + c$, where $m$ is the slope and $c$ is the intercept.\n\n![](images/linear-regression-1.png)\n\nIn the above diagram, the blue dots we see are the distribution of `y` with respect to `x`. There is no straight line that runs through all the data points. So, the objective here is to fit the best fit of a straight line that will try to minimize the error between the expected and actual value.\n\nRead more:\n- [What is ordinary least square model?](ordinary-least-square-model.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/machine-learning":{"title":"What is machine learning?","content":"\nMachine learning is the scientific study of algorithms and statistical models that computer systems use to affectively perform a specific task **without using explicit instruction**, relyring on patterns and inference instead.\n\nBuilding a model by learning the patterns of historical data with some relationship between data to make a data-driven prediction.\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n# Training Data\n\n\u003e The relationship between the data and the machine learning model, is as critical as the fuel to the engine of rocket.\n\n\u003e It is not exaggerating to say that the data dictates how the machine learning model is built.\n\n## garbage in, garbage out\n\nGroup of blind men, who have never come across an elephant before, would like to learn and conceptualize what an elephant is like by touching it. \n\nEach man touches a part of the body, such as leg, tusk or tail _etc_. While each of them got a part of the reality, none of them has the whole picture of an elephant. Therefore, **none of them actually learned the true image of an elephant**.\n\n![](https://assets.leetcode.com/uploads/2018/12/16/card_elephant.png)\n\nThe training data we got could be those images of legs or tusks from an elephant, while during the test processing, the testing data we got are the full portraits of elephants. \n\nIt would not be surprising to find out that our trained model does not perform well in this case, since we do not have the _**high-quality**_ training data that is closer to the reality in the first place.\n\nIf the data is really important, then why not feeding the \"high-quality\" data such as full portraits of elephants into the algorithm, instead of snapshots on parts of the elephant body?\n- Because, facing a problem, we or the machine, like the \"blind-men\", **often struggle to gather the data** that captures the essential characteristics of the problem, either due to the technical issues (_e.g._ data privacy) or simply because we do not perceive the problem in the right way.\n\n## Real world data\n\nIn the real world, the data we got reflects a part of reality in a favorable case, or it could be some noise in a less favorable case, or in the worst case, even a contradiction to the reality. \n\nRegardless of the machine learning algorithms, one would not be able to learn anything from data that contains too much noise or is too inconsistent with the reality.\n\n# Workflow\n\n\u003e The relationship between the data and the machine learning model, is as critical as the fuel to the engine of rocket.\n\n## Data-Centric Workflow\n\n![](https://assets.leetcode.com/uploads/2018/11/25/ml_workflow.pngthe outermost interface that we interact with the model)\n\n# Reference\n\n- https://assets.leetcode.com/uploads/2018/12/16/card_elephant.png","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/machine-learning-algorithm":{"title":"machine-learning-algorithm","content":"\nA machine learning algorithm is the process that uncovers the underlying relationship whith the data\n\nThe outcome of a machine learning algorithm is called [[machine-learning-model]], which can be considered as a `function` $F$, which outputs certain results, when given the input.\n\nReference:\n- https://leetcode.com/explore/featured/card/machine-learning-101/287/what_is_ml/1617/","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/machine-learning-model":{"title":"machine-learning-model","content":"\nRather than a predifined and fixed function, a machine learning model is dedrived from historical data. Therefore, when fed with with different data, the output of [[machine-learning-algorithm]] changes, i.e the machine learning model changes.\n\nReference:\n- https://leetcode.com/explore/featured/card/machine-learning-101/287/what_is_ml/1617/","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/mean-square-error":{"title":"What is mean square error?","content":"\nThe mean squared error tells you how close a regression line is to a set of points. It does this by taking the distance from the points to the regression line (these distance are the \"errors\") and squaring them.\n\nGiving an intuition\n\n![](images/mean-square-error-1.png)\n\nThe line equation in `Y = mX + c`. We want to find `m` (slope) and `c` (intercept) that minimize the squared error.\n\n$$\nMSE = \\frac{1}{n} \\displaystyle\\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2\n$$\n\nRead more: [Linear Regression](linear-regression.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/ordinary-least-square-model":{"title":"What is ordinary least square model?","content":"\nOLS (Ordinary Least Square) is a stats model, which will help us in identifying the more significant features that can has an influence on the output.\n\nOLS model in python is executed as:\n\n```python\nlm = smf.ols(formula='Sales ~ am + constant', data=data).fit()\nlm.conf_int()\nlm.summary()\n```\n\nand we got the output as below,\n\n![](images/ordinary-least-square-model-1.png)\n\nThe higher the `t-value` for the feature, the more significant the feature is to the output variable. The `p-value` plays a rule in rejecting the null hypothesis. If the `p-value` is less than `0.05` (95% confidence interval) for a feature, then we can consider the feature to be significant.","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/overfitting":{"title":"overfitting","content":"\n\u003e An overfitting model is the one that fits well with the training data, _i.e._ little or no error, however it does not generalized well to the unseen data\n\nHow to avoid overfitting?\n- try out another algorithm that could **generate a simpler model** from the training data set\n- adds a [[regularization]] term to the algorithm, _i.e._ penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data\n","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/r-squared":{"title":"What is R squared?","content":"\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. \n\nR-squared explained `variation / total variation`. R-squared is always between 0% and 100%. 0% indicates that the model explains none of the variability of the response data around it's mean. 100% indicates that the model explains all the variability of the response data around it's mean. In general, the higher the R-squared, the better the model fits your data.\n\n$$\nR^2 = 1 - \\frac{SS_{Regression}}{SS_{Total}} = \\frac{\\sum_i{(y_i - \\hat{y_i})^2}}{\\sum_i{(y_i - \\bar{y_i})^2}}\n$$\n\nThere is a problem with the R-squared. The problem araises when we ask this question to ourselves. \"Is it good to help as many independent variable as possible?\" The answer is No, because we understood that each independent variable should have a meaningful impact. \n\nBut, even if we add independent variables which are not meaningfully. will it improve R-squared value? Yes, this is the basic problem with R-squared. How many junk independent variables or important independent variable or impactful independent variable you add to your model, **the R-squared value will always increase**. It will never decrease with the addition of a newly independent variable, whether it could be an impactful, non-impactful, or bad variable. So we need another way to measure equivalent R-squared, which panilizes our model with any junk independent variable.\n\nSo, we calculate the Adjusted R-squared with a better adjustment in the formula of generic R-squared.\n\n$$\nR^2 adjusted = 1 - \\frac{(1-R^2)(N-1)}{N-p-1}\n$$\n\nwhere, $R^2$ is the sample R-square, $p$ is the number of predictors, and $N$ is the total sample size.","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/regularization":{"title":"regularization","content":"\n\u003e penalizing the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data\n\n","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/semi-supervised-learning":{"title":"semi-supervised-learning","content":"\nThe data set is massive but the labeled sample are few\n\nTraining strategy:\n- we train a model with the labeled data, then we apply the model to predict the unlabeled data\n- first cluster the images into groups (unsupervised learning), and then apply the supervised learning algorithm on each of the groups individually\n\n\n\n# Reference","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/supervised-learning":{"title":"supervised-learning","content":"\n\u003e An important measurement for supervised learning algorithms, is the _**generalization**_, which measures how well that a model derived from the training data can predict the desired attribute of the unseen data.\n\n","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/support-vector-regression":{"title":"What is support vector regression?","content":"\nWhy SVR (support vector regression)? What is the difference between SVR and a simple regression model?\n\nThe answer is, simple linear regression try to minimize the error rate. But in SVR, we try to fit the error within a certain threshold.\n\n![](images/support-vector-regression-1.png)\n\nMain concept:\n- Boundary Line (Red Line)\n- Kernel\n- Support Vector\n- Hyperplane (Blue Line)\n\nOur best fit line is the one where the hyperplane has the maximum number of points. What we are trying to do here is to decide a decision boundary at \"e\" distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are whithin that boundary line.\n\n\n![](images/support-vector-regression-2.png)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/tags-system":{"title":"Tags System","content":"\n# PARA Methods\n\nP.A.R.A. stands for **Projects — Areas — Resources — Archives**, the four top-level categories that encompass every type of information you might encounter in your work and life.\n\n- Projects: a series of tasks linked to a goal\n- Area: a sphere of activity with a standard to be maintained over time\n- Resource: a topic or theme of ongoing interest\n- Archive: inactive items from the other three categories\n\nexample tags:\n- project-project_name\n- area-ds\n\nResource:\n- [fortelabs](https://fortelabs.com/blog/para/)","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/tags/authors-ali-abdaal":{"title":"","content":"","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/underfitting":{"title":"underfitting","content":"\n\u003e An underfitting model is the one that does not fit well with the training data, _i.e._ significantly deviated from the ground truth\n\nCauses:\n- the model is over-simplified for the data, therefore it is not capable to capture the hidden relationship within the data\n\n![](https://assets.leetcode.com/uploads/2019/01/01/underfitting.png)\n\n\u003e Read also: [[underfitting-overfitting]]","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null},"/underfitting-overfitting":{"title":"underfitting-overfitting","content":"\n\u003e When we say a model is underfitting or overfitting, it implies that the model does not generalized well to the unseen data.\n\nwhy a model that fits well with the training data does not necessarily imply that it would generalize well to the unseen data? because...\n- **the training data are just samples we collect from the real world**, which represents only a proportion of reality. It could be the case that the training data is simply not representative, thus even the model fits perfectly the training data, it would not fit well with the unseen data\n- **the data that we collect contains noises and errors inevitably**. The model that fits perfectly with the data, would also capture the undesired noises and errors by mistake, which would eventually lead to bias and errors in the prediction for the unseen data\n\n![](https://assets.leetcode.com/uploads/2019/01/01/underfitting.png)\n\n\u003e Read also: [[underfitting]]","lastmodified":"2022-11-21T09:39:28.106329262Z","tags":null}}