{"/":{"title":"Aradinka | Digital Garden","content":"\nThis website meant to be my digital garden built using [Quartz](https://github.com/jackyzha0/quartz). Start exploring it from [what is](/tags/what-is) tag.\n\nCheck out my portfolio **[aradinka.vercel.app](https://aradinka.vercel.app/)**","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/_areas":{"title":"_areas","content":"\n[Productivity](/tags/areas-productivity)","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/_authors":{"title":"_authors","content":"\n[Ali Abdaal](/tags/authors-ali-abdaal)","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/difference-between-ai-ds-ml-dl":{"title":"What is the difference between AI, DS, ML and DL?","content":"\n## Artificial Intelligence\n\n**AI is purely math and scientifc exercise**, but when it became computational, it started to solve human problems formalized into [[a subset of computer science]].\n\nAI has changed the original computational statistics paradigm to the modern idea that machines could mimic actual human capabilities, such as decision making and performing more \"human\" tasks.\n\nModern AI into two categories:\n1. **General AI**: Planning, decision making, identifying objects, recognizing sounds, social \u0026 business interactions\n2. **Applied AI**: Driverless / Autonomous car or machine smartly trade stocks\n\n## Machine Learning\n\nInstead of engineers \"teaching\" or programming computers to have what they need to carry out tasks, that perhaps computers could teach themselves - **learn something without being explicitly programmed to do so**.\n\n**ML (Machine Learning) Is a form of AI where based on more data**, and they can change actions and response, which will make more efficient, adaptable and scalable. E.g., navigation apps and recomendation engines.\n\nML Classifiec into:\n1. Supervised\n2. Unsupervised\n3. Reinforcement Learning\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)\n\n## Data Science\n\nData science has many tools, techniques and algorithms called from these fields to handle big data. The goal of data science, somewhat similar to machine learning, is to make accurate predictions and to automate and perform transactions in real-time, such as purchasing internet traffic or automatically generating content.\n\nData science relies less on math and coding and more on data and building new systems to process the data. Relying on the fields of data integration, distributed architechture, automated machine learning, data visualization, data engineering and automated data-driven decisions. Data science can cover an entire spectrum of data pprocessing, not only the algorithms or statistics related to data.\n\n## Deep Learning\n\nDeep learning is a technique for implementing ML. ML provides the desired output from a given input, but DL (Deep Learning) reads the input and applies it to another data. \n\nIn ML, we can easily classify the flower based upon the features. Suppose you want a machine to look at an image and determine what it represents to human eye, wherer a face, flower, landscape, truck, building, etc. ML is not sufficient for this task because machine learning can only produce an output from a data set - whether according to a known algorithm or based on the inherent structure of the data. You might be able to use machine learning to determine to use ML to determine whether an image was of an \"X\" - a flower, say - and it would learn and get more accurate. But that output is binary (yes/no) and is dependent on the algorithm, not the data.\n\nIn the image recognition case, the outcome is not binary and not dependent on the algorithm. The neural network perform micro calculations with computational on many layers. Neural networks also support weighting data for confidence. These results in  a probabilistic system, vs. deterministics, and can handle tasks that we think of as requiring more \"human-like\" judgement.Z\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/difference-between-supervised-unsupervised-reinforcement-learning":{"title":"What is the difference between supervised, unsupervised, and reinforcement learning?","content":"\n## Supervised Learning\n\nIn a supervised learning model, the algorithm learns on **a labeled dataset**, to generate reasonable predictions for the response to new data. Ex:\n- Regression\n- Classification\n\n## Unsupervised Learning\n\nAn unsupervised learning model in contrast provides unlabelled data that the algorithm tries to make sense of by extracting features, co-occurrence and underlying patterns on its own. Ex:\n- Clustering\n- Anomaly detection\n- Associations\n- Autoencoders\n\n## Reinforcement Learning\n\nReinforcement learning is less supervised and depends on the learning agent in determining the output solutions by arriving at different possible ways to achieve the best possible solution.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)\n","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/general-architechture-of-machine-learning":{"title":"General Architechture of Machine Learning","content":"\n![](/images/general-architechture-of-machine-learning-1.png)\n\n**Business Understanding**: Understand the give use case and also it's good to know more about the domain for which the use cases are built.\n\n**Data Acquisition and Understanding**: Data gathering from different sources and understanding the data. Cleaning the data, handling the missing data if any, data wrangling, and EDA (Exploratory Data Analysis)\n\n\u003e Read More: [My Sample Material EDA Course on Sepuluh Nopember Institut of Technology](https://aradinka.vercel.app/blog/post-asdos)\n\n**Modeling**: Feature engineering, scaling the data, feature selection. Backward elimination method, correlation factors, PCA, and domain knowledge to select the features. Model training (Based on trial and error method of by experience, we select the algorithm and train with selected features). Model evaluation (Accuracy of the model, confusion matrix and cross-validation).\n\n**Deployment**: Once the model has good performance, we deploy the model in the cloud. Once we deploy, we monitor the performance of the model. If it's good, we go live with the model or reiterate all the process until our model performance is good.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/great-videos":{"title":"great-videos","content":"\n[inside-ali-abdaal-second-brain](inside-ali-abdaal-second-brain.md)\n","lastmodified":"2022-11-13T13:53:39.531626924Z","tags":null},"/inside-ali-abdaal-second-brain":{"title":"inside-ali-abdaal-second-brain","content":"\nhttps://youtu.be/-Y_6U0FoqDk\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Y_6U0FoqDk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/l1-regularization":{"title":"What is l1 regularization?","content":"\nThe main objective of creating a model (training data) is making sure it fits the data properly and reduce the loss. Sometimes the model that is trained may fail and give a poor performance during analyzing the test data. This leads to overfittng. Regularization came to overcome overfitting.\n\nLasso regression (Least Absolute Shrinkage and Selection Operator) adds \"Absoule value of magnitude\" of coefficient, as penalty term to the loss function. Lasso shrinks the less important features coefficient to zero, thus removing some feature altogether. So this works well for feature selection in case we have a huge number of features.\n\n\nMethods like cross-validation, stepwise regression are there to handle overfitting and perform feature selection work well with a small set of features. There techniques are good when we are dealing with a large set of features.\n\nAlong with shrinking coefficients, the lasso performs feature selection as well. Because some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/l2-regularization":{"title":"What is l2 regularization?","content":"\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn't perform well on new/unseen data on which model wasn't trained on. To avoid overfitting, we do cross-validation sampling, reducing the number of features, pruning, regularization, etc.\n\nThe regressioin model that uses L2 regularization is called ridge regression. Regularization adds the penalty as model complexity increases. The regularization parameter ($\\lambda$) penalizes all the parameters except intercept so that the model generalized the data and won't overfit.\n\nRidge regression add \"squared magnitude of the coefficient\" as penalty term to the loss function. If the $\\lambda$ is zero, then it is equivalent to [OLS](ordinary-least-square-model.md). But if the lambda is very large, then it will add too much weight, and it will lead to underfitting.\n\nRidge regularization forces the weights to be small but doesn't not make them zero, and doesn't give the sparse solution. Ridge is not robust to outliers as square terms blow up the error differences of the outliers, and the regularization term tries to fix it by penalizing the weights.\n\nRidge regression performs better when all the input features influence the output, and all with weights are of roughly equal size. L2 regularization can learn complex data patterns.\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/linear-regression":{"title":"What is linear regression?","content":"\n## Linear Regressions\n\nLinear regressions tends to establish a relationship between a dependent variable `Y` and one or more independent variable `X` by finding the best fit of the straight line.\n\nThe equation for linear model is $Y = mX + c$, where $m$ is the slope and $c$ is the intercept.\n\n![](images/linear-regression-1.png)\n\nIn the above diagram, the blue dots we see are the distribution of `y` with respect to `x`. There is no straight line that runs through all the data points. So, the objective here is to fit the best fit of a straight line that will try to minimize the error between the expected and actual value.\n\nRead more:\n- [What is ordinary least square model?](ordinary-least-square-model.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/machine-learning":{"title":"What is machine learning?","content":"\nMachine learning is the scientific study of algorithms and statistical models that computer systems use to affectively perform a specific task **without using explicit instruction**, relyring on patterns and inference instead.\n\nBuilding a model by learning the patterns of historical data with some relationship between data to make a data-driven prediction.\n\n\u003e Read more: [What is the difference between supervised, unsupervised, and reinforcement learning?](difference-between-supervised-unsupervised-reinforcement-learning.md)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/mean-square-error":{"title":"What is mean square error?","content":"\nThe mean squared error tells you how close a regression line is to a set of points. It does this by taking the distance from the points to the regression line (these distance are the \"errors\") and squaring them.\n\nGiving an intuition\n\n![](images/mean-square-error-1.png)\n\nThe line equation in `Y = mX + c`. We want to find `m` (slope) and `c` (intercept) that minimize the squared error.\n\n$$\nMSE = \\frac{1}{n} \\displaystyle\\sum\\limits_{i=1}^n (y_i - \\hat{y_i})^2\n$$\n\nRead more: [Linear Regression](linear-regression.md)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/ordinary-least-square-model":{"title":"What is ordinary least square model?","content":"\nOLS (Ordinary Least Square) is a stats model, which will help us in identifying the more significant features that can has an influence on the output.\n\nOLS model in python is executed as:\n\n```python\nlm = smf.ols(formula='Sales ~ am + constant', data=data).fit()\nlm.conf_int()\nlm.summary()\n```\n\nand we got the output as below,\n\n![](images/ordinary-least-square-model-1.png)\n\nThe higher the `t-value` for the feature, the more significant the feature is to the output variable. The `p-value` plays a rule in rejecting the null hypothesis. If the `p-value` is less than `0.05` (95% confidence interval) for a feature, then we can consider the feature to be significant.","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/r-squared":{"title":"What is R squared?","content":"\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. \n\nR-squared explained `variation / total variation`. R-squared is always between 0% and 100%. 0% indicates that the model explains none of the variability of the response data around it's mean. 100% indicates that the model explains all the variability of the response data around it's mean. In general, the higher the R-squared, the better the model fits your data.\n\n$$\nR^2 = 1 - \\frac{SS_{Regression}}{SS_{Total}} = \\frac{\\sum_i{(y_i - \\hat{y_i})^2}}{\\sum_i{(y_i - \\bar{y_i})^2}}\n$$\n\nThere is a problem with the R-squared. The problem araises when we ask this question to ourselves. \"Is it good to help as many independent variable as possible?\" The answer is No, because we understood that each independent variable should have a meaningful impact. \n\nBut, even if we add independent variables which are not meaningfully. will it improve R-squared value? Yes, this is the basic problem with R-squared. How many junk independent variables or important independent variable or impactful independent variable you add to your model, **the R-squared value will always increase**. It will never decrease with the addition of a newly independent variable, whether it could be an impactful, non-impactful, or bad variable. So we need another way to measure equivalent R-squared, which panilizes our model with any junk independent variable.\n\nSo, we calculate the Adjusted R-squared with a better adjustment in the formula of generic R-squared.\n\n$$\nR^2 adjusted = 1 - \\frac{(1-R^2)(N-1)}{N-p-1}\n$$\n\nwhere, $R^2$ is the sample R-square, $p$ is the number of predictors, and $N$ is the total sample size.","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/support-vector-regression":{"title":"What is support vector regression?","content":"\nWhy SVR (support vector regression)? What is the difference between SVR and a simple regression model?\n\nThe answer is, simple linear regression try to minimize the error rate. But in SVR, we try to fit the error within a certain threshold.\n\n![](images/support-vector-regression-1.png)\n\nMain concept:\n- Boundary Line (Red Line)\n- Kernel\n- Support Vector\n- Hyperplane (Blue Line)\n\nOur best fit line is the one where the hyperplane has the maximum number of points. What we are trying to do here is to decide a decision boundary at \"e\" distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are whithin that boundary line.\n\n\n![](images/support-vector-regression-2.png)\n\n\u003e Source: [iNeuronai](https://github.com/iNeuronai/interview-question-data-science-)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/tags-system":{"title":"Tags System","content":"\n# PARA Methods\n\nP.A.R.A. stands for **Projects — Areas — Resources — Archives**, the four top-level categories that encompass every type of information you might encounter in your work and life.\n\n- Projects: a series of tasks linked to a goal\n- Area: a sphere of activity with a standard to be maintained over time\n- Resource: a topic or theme of ongoing interest\n- Archive: inactive items from the other three categories\n\nexample tags:\n- project-project_name\n- area-ds\n\nResource:\n- [fortelabs](https://fortelabs.com/blog/para/)","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null},"/test-notes":{"title":"Test Notes","content":"","lastmodified":"2022-11-13T13:53:39.543627111Z","tags":null}}